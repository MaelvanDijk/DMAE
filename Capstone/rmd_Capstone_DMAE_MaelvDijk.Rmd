---
title: "Capstone_MaelvDijk_DMaE"
author: "Maël"
date: '2022-03-15'
output: 
  html_document:
    toc: true
    toc_float: true
    highlight: tango
    theme: journal
    code_folding: hide
    # number_sections: true
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE
  )

library("tidyverse")
library("stringr")
library("tseries")


#custom functions
source('./supporting_code/Scraped_indeed_data_wrangler.R')
source('./supporting_code/Scraped_indeed_plotter.R')
source('./supporting_code/Kaggle_indeed_plotter.R')
```

## Samenvating

*Samenvatting (max. 250 woorden)*

HoB blijkt vanaf haar 3e jaar een competatief salaris te bieden waarin
het meer dan 50% van de markt verslaat. In de eerste 2 jaren verdienen
werknemers boven het modale loon. Het blijkt echter dat House of Bèta
nog ruimte heeft om de hard skills van haar personeel te ontwikkeling.
Het belangrijkste zijn skills m.b.t de cloud, big data, en geavanceerde
data science skills. Op het moment van schrijven staan de volgende
potentiële opdrachtgevers boven aan de lijst met openstaande vacatures:

## Inleiding

Dit onderzoek richt zich op de vraag hoe competitief House of Bèta is op
de markt voor data professionals. Hierin wordt gekeken naar zowel het
salaris (dus wat House of Bèta biedt aan het personeel) als de
mogelijkheden voor House of Bèta om een groter deel van de markt te
bedienen (gevraagde vaardigheden en potentiële opdrachtgevers).

House of Bèta heeft al langer de wens om inzichten te krijgen in de
reden van uitstroom van haar consultants, gezien dit een belangrijke
asset is van de organisatie. Wegens juridische en technische beperkingen
is het echter niet mogelijk om hierop diepgaand onderzoek op te doen.

Een tweede wens is om de markt beter te kunnen bedienen en te groeien
als de leverancier van IT personeel. Na overleg is gebleken dat beide
wensen gekoppeld kunnen worden in dit onderzoek. Het onderzoek helpt
House of Bèta inzichten te krijgen in een ontwikkelende markt, haar
eigen arbeidsvoorwaarden op waarden te schatten en biedt een basis voor
verdere analyse aangaande de churn van haar personeel.

### House of Bèta

House of Bèta (HoB) is een detacheerder op het gebied van (business) IT.
HoB probeert voornamelijk starters op de arbeidsmarkt aan te trekken en
probeert deze te matchen aan passende opdrachtgevers. De consultants
hebben de mogelijkheid zich na verloop van tijd te specialiseren waarin
HoB de trainingen en opleiding faciliteert of betaald.

HoB is een relatief nieuwe naam op de markt maar voert al sinds 2015
haar werkzaamheden uit onder het moederbedrijf Talent&Pro. De uitstroom
van consultants die T&P heeft gezien over2021 is de aanleiding geweest
om meer grip op deze uitstroom te willen krijgen (de eerste wens).

Tijdens de een recente informatieavond van HoB zijn nieuwe
doelstellingen voor 2022 gepresenteerd waar ook de ontwikkeling en groei
van data professionals naar voren komt. Dit is dan ook de aanleiding
voor de tweede wens van HoB.

### Businessvraag

Dit onderzoek en de wensen van HoB kunnen worden vastgelegd in een
centrale vraag: "Hoe positioneert HoB zich ten opzichten van de
Nederlandse arbeidsmarkt op het gebied van data professionals kijkend
naar loon, vaardigheden en potentiële opdrachtgevers?"

Deze vraag kan verder worden opgedeeld in 3 deelvragen:

1\. Hoe verhoudt het salaris van HoB zich tot de Nederlandse markt?

2\. In hoeverre sluit het "curriculum" van HoB zich aan bij de
ontwikkelende markt?

3\. Welke opdrachtgevers actief op de markt worden nog niet bediend door
HoB?

### data gebruik

Binnen HoB vinden nog geen datamining activiteiten op dit gebied plaats.
Buiten de resultaten van dit onderzoek wordt er dan ook een een script
aangeleverd die het mogelijk maakt om de datamining voor dit onderzoek
voort te zetten. Dit script is beperkt tot het zoekwoord "data" op de
vacaturesite Indeed, maar kan uitgebreid worden om op andere woorden te
zoeken.

### Business Success criteria

De volgende kwalitatieve eisen worden gesteld: er moet er een analyse
komen van HoB versus markt inkomen, een analyse over de vraag verdeling
over bedrijven en een over skills. Daarnaast moet er een bruikbare
scraper voor vervolg onderzoek worden opgeleverd.

## Methoden

Dit hoofdstuk behandelt de data en het gebruik hiervan. In "data
understanding" worden basale kenmerken van de dataset besproken, "data
preperation en modelling" gaat in op het schonen een verrijken van de
data. Als laatste gaat "reproduceren van het onderzoek" over de
mogelijkheid om dit onderzoek na te bootsten.

### Data understanding

Hieronder worden de scope, de gebruikte datasets en de basale kenmerken
beschreven.

#### Gekozen datasets en scope

Om te beginnen is hieronder een versimpelde datalog weergegeven. Dit
bevat informatie over de datasets in de ruwe vorm (te vinden in
".\\Data_raw")".

| ID   | dataset name                   | format | beschrijving                                                                         | dataset owner |
|----------|----------|----------|--------------------------------|----------|
| D001 | indeed_job_dataset.csv         | CSV    | dataset met job post data vanuit de VS voor data anlysten, scientisten en engineers. | Elroy         |
| D002 | <DATE> scraped indeed data.csv | CSV    | scraped data voor functies met het woord "data" in nederland                         | Maël          |

: datalog (shortened)

D001 is na vooronderzoek gekozen als basis dataset. Hierin staan onder
andere (hard)skills, functie type (data analist, -engineer, -scientist).
Dit legt een goede basis voor het onderzoek naar data specialisaties
binnen HoB, hier worden namelijk dezelfde specialisaties ook aangeboden.
Er zijn andere datasets overwogen echter bevatte deze te weinig
relevantie functie type (\< 200 rijen) of ontbraken hier relevante
skills.

D002 is gescrapte data van Indeed. Het rationaal achter het zelfstandig
scrapen van Indeed data komt voort uit het tekortschieten van bestaande
API's en afwezigheid van relevante datasets.

#### Beschrijving van de ruwe data

Beginnend bij de beschrijving van D001. Deze dataset is al erg
opgeschoond. Een aantal basale kenmerken zijn hieronder programmatische
weergegeven. Allereerst is duidelijk dat er 43 kolommen aanwezig zijn.
Echter voor het doel van dit onderzoek zijn maar twee kolommen echt
interessant. Dit zijn "Skill" en "Job_Type". Hierin staan
respectievelijk een lijst van (hard)skills en de functie soort per
observatie.

```{r basic D001 data, echo=FALSE}
# kenmerken D001 dataset
df_indeed_kaggle <- read.csv(".\\Data_raw\\D001\\indeed_job_dataset.csv")

# head(df_indeed_kaggle)
# df_indeed_kaggle %>% summary()
writeLines(
  sprintf(
    "Number of columns in the dataset:\n%s\n",
    ncol(df_indeed_kaggle)
  )
)

writeLines(
  sprintf(
    "Number of rows in the dataset:\n%s\n",
    nrow(df_indeed_kaggle)
  )
)

writeLines(
  sprintf(
    "Number of Skill (combinations) in the dataset:\n%s\n",
    df_indeed_kaggle$Skill %>% unique() %>% length()
  )
)

writeLines(
  sprintf(
    "Number of salaries  in the dataset:\n%s\n",
    df_indeed_kaggle$Queried_Salary %>% unique() %>% length()
  )
)

writeLines(
  sprintf(
    "number of observations per job type (analist, engineer, scientist):\n%s",
    aggregate(
      df_indeed_kaggle$Job_Type,
      by=list(df_indeed_kaggle$Job_Type),
      FUN=length
      )[2]
  )
)


```

Zoals hierboven getoond komt duidelijk naar voren dat het aantal unieke
"Skills" bijna gelijk is aan het aantal unieke rijen. Dit betekent
echter niet dat er zoveel verschil zit tussen de vacatures. Het is
namelijk zo dat [SQL, Python] en [Python, SQL] hetzelfde betekenen maar
wordt gezien als twee unieke rijen. Deze informatie wordt gebruikt in
latere ETL stappen. Daarnaast is opvallend dat de verdeling tussen data
analist (31%), data engineer (24%) en data scientist (45%) niet volledig
evenredig is.

Dataset D002 bevat datavan Indeed. Hiervoor is er gezocht naar alle
functies waar het woord "data" in voorkomt. D002 bestaat in de ruwe vorm
uit meerdere losse bestanden (een voor elke dag dat er een run is
geweest van de scraper). Om een indruk te krijgen van wat er in een
dergelijk bestand staat kan er gekeken worden naar
".\\Data_raw\\D002\\2022-03-08 scraped indeed data.csv". Hieronder
worden weer de voornaamste kenmerken.

```{r basic D002 data, echo=FALSE}
# kenmerken D002 dataset
df_indeed_scraped <- read.csv(".\\Data_raw\\D002\\2022-03-08 scraped indeed data.csv")

writeLines(
  sprintf(
    "Number of columns in the dataset:\n%s\n",
    ncol(df_indeed_scraped)
  )
)

writeLines(
  sprintf(
    "Number of rows in the dataset:\n%s\n",
    nrow(df_indeed_scraped)
  )
)

writeLines(
  sprintf(
    "number of unique skill(s) and combinations:\n%s\n",
    df_indeed_scraped$skills %>% unique() %>% length()
  )
)

writeLines(
  sprintf(
    "number of unique URL's:\n%s\n",
    df_indeed_scraped$job_link %>% unique() %>% length()
  )
)

writeLines(
  sprintf(
    "Example of salary:\n%s\n",
    df_indeed_scraped$salary[2]
  )
)

writeLines(
  sprintf(
    "number of NA's:\n%s\n",
    sum(is.na(df_indeed_scraped))
  )
)

writeLines(
  sprintf(
    "earliest job listing:\n%s\n",
    df_indeed_scraped$listing_date %>% min(na.rm = TRUE)
  )
)

writeLines(
  sprintf(
    "Skills example:\n%s\n",
    df_indeed_scraped$skills[2]
  )
)
```

Het is gelijk duidelijk dat de data niet verder terug gaat dan 6
februari 2022. Dit zal effect hebben op de trend analyses in dit
onderzoek. Ook wordt duidelijk dat een dataset meer rijen heeft dan
unieke URL's. Dit betekent dat dezelfde URL waarschijnlijke meerdere
keren is gescraped. Het derde feit dat duidelijk wordt, euro symbolen
(€) worden foutief weergegeven (ï¿½). Dit en andere
datakwaliteitsproblemen worden in het ETL proces opgelost.

### Data preperation en modeling

Onderstaande processflow geeft in grote lijnen weer hoe er vanaf de ruwe
data wordt gekomen tot een eindproduct.

Het volledige proces is in drie segmenten op te delen. Bovenin betreft
D001. Segment twee (midden) betreft D002. Onderin staat segment drie
waarin het eindresultaat weergegeven wordt.

![Data process flow](Images/data%20flow%20process.png)

Omdat de tijd en scope beperkt is voor dit onderzoek zijn er een aantal
aannames gedaan om problemen en complexiteit te voorkomen:

| Process stap        | Betreffende kolom | Versimpelde weergaven                                                                                                                                                                                                                |
|----------|----------|---------------------------------------------------|
| Scraping            | Salary            | Er is bij weergaven salarissen altijd voor het laagste salaris gegaan                                                                                                                                                                |
| ETL (midden-rechts) | job_type          | Er zijn 3 type (analist, engineer, scientist). De dataset wordt verrijkt met een van deze type zelfs al wijkt dit af. Zo kan de functie "data entry" als "data analist" worden geclassificeerd.                                      |
| ETL (midden-rechts) | salary            | Salaris \> 10.000 is jaarsalaris of salaris \< 100 is uur tarief. Dit wordt omgerekend naar maandsalaris (delen door 12 of maal 168 respectievelijk). Alles tussen de 100 en 1000 wordt als scraping fout gezien en omgezet naar NA. |

: Aannamens tijdens process stappen,

De scripts voor deze stappen zijn te vinden in "./Supporting_code":

| Process stap        | Script naam                    |
|---------------------|--------------------------------|
| ETL (linksboven)    | Generate_ref_skill_list.R      |
| Scraping            | indeed_scraper.R               |
| ETL (midden-rechts) | Scraped_indeed_data_wrangler.R |

: process stappen en bijbehorende scripts.

De scripts en betreffende functies zorgen voor een dataset die gebruikt
kan worden voor dit onderzoek. De scripts zijn voorzien van helpende
comments om het gebruik te vergemakkelijken.

### Reproduceren van het onderzoek

De mappen: "Data_cleaned", "Data_raw", en "Supporting_code" bevatten
alle data en code die nodig is om dit onderzoek te reproduceren. In dit
markdown document worden de data en benodigde scripts aangeroepen om tot
het eindresultaat te komen. De benodigde R libraries zijn: "tidyverse",
"stringr", "lubridate", "tibble", "textTinyR", "XML", "rvest",
"tseries".

## Resultaten

---
# Resultaten
# 
#     Sluit aan bij Evaluation van CRISP-DM
#     Presenteer de resultaten die nodig zijn voor het beantwoorden van de onderzoeksvragen
#     Ondersteun de tekst met figuren en tabellen
---

Dit hoofdstuk beantwoord de deelvragen aan de hand van diverse analyses.
Ook wordt besproken welke vragen (nog) niet kunnen worden beantwoord en
welke artefacten worden aangeleverd.

```{r clean and enrich D002, echo=FALSE}
# clean en verrijk indeed data met custom functies
df_indeed_merged <- merge_scraped_data()
df_indeed_cleaned <- clean_scraped_date(df_indeed_merged)

# geschoonde en verrijkte dataset
df_indeed_final <- enrich_scraped_date(df_indeed_cleaned)

# dataset voor skill analyse
df_indeed_skills <- unnest_skills(df_indeed_final)

# dataset voor tijdserie analyse
df_indeed_final_date_filtered <- filter_dates(df_indeed_final)
df_indeed_skills_date_filtered<- filter_dates(df_indeed_skills)

rm(
  df_indeed_kaggle,
  df_indeed_scraped,
  df_indeed_merged,
  df_indeed_cleaned
)
```

### Hoe verhoudt het loon van HoB zich tot de Nederlandse banenmarkt op het gebied van data specialisten? {.tabset}

HoB biedt verschillende manieren om salaris verhoging te halen, dit
tezamen met de gesloten HR systemen maakt het lastig om een referentie
salaris te bepalen. In dit geval heeft HoB op haar website aangegeven
wat een collega kan verdienen in jaar 1, 2 en 3 (€2.763,- , €3.087,- ,
€3.591,- respectievelijk). Om deze deelvraag te beantwoorden worden deze
drie salarissen gebruikt.

De grafiek hieronder geeft de salaris verdeling van data vacatures op
Indeed weer. Het eerste wat duidelijk wordt is dat het loon van HoB bij
aanvang al boven modaal ligt. Daarnaast is het salaris in jaar 3 hoger
dan het mediaan salaris op Indeed. Ofwel, HoB biedt een hoger
maandsalaris in jaar 3 dan 50% van de markt (afgaande op Indeed).

```{r plot indeed salary distribution vs hob salary, echo=FALSE}
plot_hob_salary_vs_indeed_salary_dist(df_indeed_final)
```

Bovenstaande analyse zegt echter niet alles. Hierin wordt het 3
verschillende maandsalarissen van HoB vergelijken met de gehele
onderzoekspopulatie. Op Indeed worden diverse ervaringseisen gesteld.
Dit gebeurt in de vorm van "1 -- 3 jaar ervaring" o.i.d. Tijdens het
schonen is altijd de kleinste hoeveelheid opgeslagen (1 -- 3 is
opgeslagen als 1).

Hieronder is de salaris verdeling te zien gesegmenteerd op gevraagde
ervaring. Door de dataset op te delen in negen segmenten (gemiddeld 166
observaties) is de verdeling gevoeliger en minder accuraat. Toch geeft
het de indruk dat het salaris geboden door HoB doorgaans beter is dan
het salaris van Indeed (jaren 6 en 7 buiten beschouwing gelaten).

```{r HoB salaris vs Indeed verdeling per ervaringsjaar, echo=FALSE}
df_indeed_final %>%
    ggplot(aes(x= salary)) +
    geom_density(alpha=0.5) +
    geom_segment(
      aes(
        x =2763, xend =2763,
        y= 0.0000, yend=0.003,
        linetype= "HoB 1ste jaar"
      )
    ) +
      geom_segment(
      aes(
        x =3087, xend =3087,
        y= 0.0000, yend=0.003,
        linetype= "HoB 2de jaar"
      )
    ) +
      geom_segment(
      aes(
        x =3519, xend =3519,
        y= 0.0000, yend=0.003,
        linetype= "HoB 3de jaar"
      )
    ) +
    facet_wrap(vars(cleaned_werkervaring_jaren)) +
    labs(
      title="Verdeling geboden salaris per jaren werkervaring",
      y= "Frequentie (%)",
      x= "Geboden salaris (\u20ac)"
    ) +
    theme(
      plot.title=element_text(hjust=0.5))+
    scale_y_continuous(
      breaks= seq(0.0000, 0.004, 0.001),
      labels=seq(0.00, 0.04, 0.01),
      limits= c(0, 0.003)
    ) +
  xlim(2000,6000)
```

Het salaris bij HoB wordt niet alleen bepaalt door werkervaring. Ook het
afronden van cursussen en het opdoen van skills wordt beloond. Wat
betreft hard skills kan er vanuit worden gegaan dat een consultant 4 tot
6 skills bezit na de eerste 3 jaar bij HoB. Dit zijn data modeling,
excel, R, Python, SQL en PowerBI. In onderstaande drie grafieken is het
mediaan salaris op Indeed weergegeven verdeeld over het gevraagde aantal
skills. De zwarte lijn geeft het bijbehorende HoB salaris aan.

De drie grafieken met elkaar vergeleken laat zien dat HoB vooral vanaf
het 3e jaar echt een competatief salaris biedt. Hierbij kan verwacht
worden dat een consultant zo'n vier tot zes vaardigheden bezit. Daarmee
verslaat het de markt kijkend naar skill,-, loonratio.

#### 1ste jaars HoB

```{r}
df_indeed_final %>%
  group_by(skill_count) %>%
  summarise(med= median(salary, na.rm = T)) %>%
  ggplot(
    aes(x=skill_count, y=med)) +
  geom_col(
    aes(
      fill = ifelse(med < 2763, 'red', 'blue')
      )
    )+
  geom_hline(linetype= 1,
             yintercept = 2763
             )+
  labs(
    title = "Mediaan salaris Indeed vs. aantal skills",
    subtitle = "HoB 1ste jaars salaris als referentie lijn",
    y = "Mediaan salaris Indeed",
    x = "Aantal skills"
  ) +
  theme(
    legend.position= "none"
  )
```

#### 2de jaars HoB

```{r}
df_indeed_final %>%
  group_by(skill_count) %>%
  summarise(med= median(salary, na.rm = T)) %>%
  ggplot(
    aes(x=skill_count, y=med)) +
  geom_col(
    aes(
      fill = ifelse(med < 3087, 'red', 'blue')
      )
    )+
  geom_hline(linetype= 1,
             yintercept = 3087
             )+
  labs(
    title = "Mediaan salaris Indeed vs. aantal skills",
    subtitle = "HoB 2de jaars salaris als referentie lijn",
    y = "Mediaan salaris Indeed",
    x = "Aantal skills"
  ) +
  theme(
    legend.position= "none"
  )
```

#### 3de jaars HoB

```{r}
df_indeed_final %>%
  group_by(skill_count) %>%
  summarise(med= median(salary, na.rm = T)) %>%
  ggplot(
    aes(x=skill_count, y=med)) +
  geom_col(
    aes(
      fill = ifelse(med < 3519, 'red', 'blue')
      )
    )+
  geom_hline(linetype= 1,
             yintercept = 3519
             )+
  labs(
    title = "Mediaan salaris Indeed vs. aantal skills",
    subtitle = "HoB 3de jaars salaris als referentie lijn",
    y = "Mediaan salaris Indeed",
    x = "Aantal skills"
  ) +
  theme(
    legend.position= "none"
  )

```

Bovenstaande analyse zou nog verdiept kunnen worden in een vervolg
onderzoek door het aantal jaren werkervaring mee te nemen. Dit levert nu
echter te weinig observaties (11) per categorie op.

### In hoeverre sluit het "curriculum" van HoB zich aan bij de ontwikkelende markt? {.tabset}

Deze deelvraag behoeft twee analyses. Enerzijds is het van belang om te
weten hoe het huidige aanbod van HoB aansluit op de huidige vraag.
Daarnaast is het van belang om te weten hoe de vraag naar Skills zich
ontwikkelt. Binnen dit onderzoek is het laatste lastig te realiseren.
Een goede trend analyse zal \~3 jaar aan data behoeven. Hierin schiet de
dataset te kort, dit zal impact hebben op de analyse.

Het eerste waar naar gekeken wordt, de huidige vraag naar skills.
Hieronder zijn twee grafieken te vinden. De eerste grafiek geeft de top
20 vaardigheden weer, de tweede grafiek de minst gevraagde 20.

#### top skills

```{r}

all_skills <- df_indeed_skills$skills %>% sort(decreasing= TRUE)

# create a top and bottom 20
top_skills <- table(df_indeed_skills$skills) %>% sort() %>% tail(20)
bottom_skills <- table(df_indeed_skills$skills) %>% sort() %>% head(20)

# plot top skills
df_indeed_skills %>%
    # filter df based on most sought skill
  filter(skills %in% names(top_skills)) %>% 
  ggplot(
      aes(
        x = reorder(
          skills,
          skills,
          function(x) + length(x) # decreasing order skill
          ),
          fill= job_type
          )
      ) +
  geom_bar() +
  theme(
    axis.text.y = element_text(
      face= 'bold',
      size= 14)) + # Rotate x-labels and change font
      coord_flip() +
  labs(
    x= "Skill"
  )


```

#### Bottom skills

```{r}

# plot bottom skills
df_indeed_skills %>%
    # filter df based on least sought skill
  filter(skills %in% names(bottom_skills)) %>% 
  ggplot(
      aes(
        x = reorder(
          skills,
          skills,
          function(x) + length(x) # decreasing order skill
          ),
          fill= job_type
          )
      ) +
  geom_bar() +
  theme(
    axis.text.y = element_text(
      face= 'bold',
      size= 14)) + # Rotate x-labels and change font
      coord_flip() +
  labs(
    x= "Skill"
  )
```

Zoals gesteld zijn de consultants in de eerste 3 jaar voornamelijk bezig
met de skills data modeling, excel, R, Python, SQL en PowerBI. Zoals te
zien sluit HoB met vier van de skills aan bij de meest gewilde skills.
Ook wordt er ingezet op cloud oplossingen binnen HoB wat aansluit op de
gevraagde skills "azure" en "aws". Tableau is een onverwachte
vaardigheid, in plaats daarvan was de verwachting namelijk dat PowerBI
in de top zou staan hier zit dus een gat in vraag en aanbod. Daarnaast
wordt duidelijk dat er op het gebied van data science een gat bestaat
tussen HoB en de markt. Zo zijn data science vaardigheden (ai, machine
learning, spark, scala, go) niet direct onderdeel van het curriculum.

De minst gevraagde skills zijn voor dit onderzoek minder relevant. Toch
is het goed om kaart te hebben welke skills het minst gevraagd worden.
Dit zijn voornamelijk skills om te vermijden.

Interessant om nog op te merken dat het voornaamste aandeel van
gevraagde skills zich rond de top 5 bevindt. Dit zijn dan ook gebieden
waar HoB zich het best op kan focussen om relevant te blijven voor de
markt. Wilt HoB uitbreiden richting minder gevraagde maar toch relevante
skills? Dan is een tweede stap om ontwikkeling te bieden in de top 20.
Dit kan voor programmeertalen en frameworks een cursus zijn, voor
concepten zoals "rest" (API's) zijn dit informatie avonden.

### Instroom vacatures {.tabset}

Een volgende stap is patroon analyse. Omdat de data niet vergenoeg terug
gaat kan er geen accurate voorspelling worden gedaan over de
ontwikkeling naar de vraag. Naast dat het inspelen op deze vraag
ontwikkeling ook erg lastig is wegens de doorlooptijd van het opzetten
van opleidingen en het opleiden van kandidaten. Wel kan er gestart
worden met een analyse of er een bepaald patroon is m.b.t. de instroom.

De eerste grafiek hieronder maakt duidelijk dat er een grote instroom
van vacatures is op het eerste moment van scrapen. Dit heeft een
logische verklaring: Indeed kent drie soorten waarden m.b.t plaatsings
moment "Vandaag", "1-30 dagen geleden" "30+ dagen geleden". De scraper
maakt geen onderscheid tussen "30" en "30+" Dit betekent dat het eerste
moment van scrapen alle "30" en "30+" kenmerken om worden gezet naar
"30". Dit verklaart een spike wanneer deze twee waarden voorkomen,
doordat tijdens het data schonen dubbele rijen worden verwijderd (de
eerste wordt behouden) zien we dit effect alleen terug in het begin van
de totale dataset. Wanneer er dus gekeken wordt naar de vraag over tijd
zal het binnen deze dataset noodzakelijk zijn om vanaf 5 februari 2022
te kijken. In beide grafieken is lastig vast te stellen of er een
correlatie of een bepaald patroon aanwezig is. Wel valt op te maken dat
er over het algemeen een dalende trend lijkt te zijn. Om hier echter
meer over te zeggen zal minstens 3 jaar aan data nodig zijn om trends en
patronen te kunnen vaststellen.

Beide grafieken lijken pieken op bepaalde intervallen te tonen. In de
derde grafiek wordt daarom de autocorrelatie weerggeven hieruit blijkt
dat er geen duidelijk patroon aanwezig is, hierbij zou 1 (of -1) een
mogelijke correlatie of patroon aanduiden. Deze vindt alleen plaats op
lag 0. Dit betekent dat er alleen een verband is op de dag zelf. Kortom
lijkt er geen patroon zichtbaar te zijn wat betreft de instroom.

#### Instroom vanaf begin

```{r unfilterd date counts}

plot_indeed_timeseries_data(df_indeed_final)

```

#### Instroom vanaf 5 februari 2022

```{r filtered date counts}

# filter data to contain only points after 5th of february


plot_indeed_timeseries_data(df_indeed_final_date_filtered)

```

#### Auto-correlatie instroom

```{r}
#create listing date table
listing_date_table <- table(df_indeed_final_date_filtered$listing_date)

#calculate autocorrelation
acf(listing_date_table, main= "Instroom autocorrelatie")
```

### Ontwikkeling skill behoeften {.tabset}

Hoewel er met de huidige data nog geen overkoepelend patroon te
signaleren is kan er wel gekeken worden naar de ontwikkeling van de
skill behoeften. Ook hier mist er een hoeveelheid data waarmee een goede
tijdanalyse gedaan kan worden. Daarom dienen onderstaande grafieken en
bijbehorende code ook als artefact waar op een later moment gebruik van
gemaakt kan worden. In onderstaande grafieken zijn in iedergeval de drie
meest gebruikte programmeertalen rondom data specialisaties te vinden.
Daarnaast is het analyse platform Tableau weergegeven en de twee cloud
oplossingen AWS en Azure.

Ook is hier weer een dalende trend te zien in nagenoeg alle skill
aanvragen. Zowel in onderstaande grafieken als in het overzicht van
totale aanvragen kan dit te verklaren zijn door de rijkwijdte van het
scraping script (max 300 vacatures) en de willekeurige sortering op de
Indeed. Daarom zal ook hier de tijd een belangrijke factor spelen en een
goede analyse na 3 jaar data verzamelen gedaan kunnen worden.

Wat we uit deze en de voorgaande analyse in iedergeval kunnen opmaken is
het feit dat er een daling is in vacatures na een piek aan het begin van
februari.

#### SQL

```{r SQL instroom}
plot_indeed_timeseries_data(df_indeed_skills_date_filtered, "sql")
```

#### Python

```{r Python instroom}
plot_indeed_timeseries_data(df_indeed_skills_date_filtered, "python")
```

#### R

```{r R instroom}
plot_indeed_timeseries_data(df_indeed_skills_date_filtered, "r")

```

#### Tableau

```{r Tableau instroom}
plot_indeed_timeseries_data(df_indeed_skills_date_filtered, "tableau")
```

#### Azure

```{r Azure instroom}
plot_indeed_timeseries_data(df_indeed_skills_date_filtered, "azure")
```

#### AWS

```{r AWS instroom}
plot_indeed_timeseries_data(df_indeed_skills_date_filtered, "aws")
```

### Ontwikkeling opdrachtgevers {.tabset}

De laatste deelvraag voor dit onderzoek betreft de ontwikkeling en
verhouding van de vraag vanuit mogelijke opdrachtgevers. Omdat het
gebleken is dat de ontwikkeling van de vraag te weinig zegt wordt hier
alleen nog gekeken naar de totale vraag vanuit opdrachtgevers. Hierop
kunnen we binnen een aantal elementen segmenteren, namelijk: functietype
en gevraagde skills.

Allereerst wordt gekeken naar de 25 bedrijven met het grootste aanbod .
in de eerste grafiek is te zien dat er overwegend gevraagd wordt naar
data scientisten, vervolgens data analisten, en in veel mindere maten
naar data engineers. Wanneer andere detacheerders / uitzendbureaus /
consultancies worden weggelaten blijft de volgende top 5 bedrijven over
(ten tijden schrijven):

1.  Bertelsmann
2.  NN
3.  Rabobank
4.  Belastingdienst
5.  Rijkswaterstaat

Vanuit bovenstaande groep werkt HoB nog niet samen met Bertelsmann (een
mediaconglomeraat) wat zou kunnen duiden op mogelijkheden tot een nieuwe
samenwerking.

In de volgende twee grafieken zijn de top 10 bedrijven (van de top 25)
gekozen die zich niet richten op detachering, consultancy of
traineeships. Hierin is de verdeling over functie type en de 10 meest
gevraagde skills te zien. Het is duidelijk te zien dat zowel de functie
type en de gevraagde vaardigheden in zekere zinnen overeenkomen met de
eerder vastgestelde verdeling over de gehele gescrapte populatie (een
paar afwijkingen achterwege gelaten). Zo lijkt de data analist het meest
gezocht te worden, gevolgd door data scientist en als laatste de data
engineers. Wat betreft vaardigheden komen de top 3 vaardigheden overeen
met de algehele populatie. Het meest opvallende binnen de vaardigheden
zijn "marketing" en "Tableau". Deze zijn niet veel vertegenwoordigd in
de vacatures geplaatst door de meest frequente opdrachtgevers.

Wanneer we echter naar de vierde grafiek kijken zien we de meest
gevraagde skills door de top 10 opdrachtgevers. Het beeld lijkt hier
vertekend door Bertelsmann omdat deze een groot deel van de aanvragen
uit heeft staan. Des al niet te min blijft de top 4 (sql, python, R,
(neural) network) ongewijzigd. Hier verder op in gaande is ook te zien
data Bertelsmann in zet op big data technologie (Hive, scala en spark).
Dit zijn interessante signalene komende van de grootst potentiele
opdrachtgever op Indeed.

#### top 20 bedrijven - functie types

```{r}

top_companies <- table(df_indeed_final$Company) %>% sort() %>% tail(25)

df_indeed_final %>%
  filter(Company %in% names(top_companies)) %>%
  ggplot(
    aes(
        x = reorder(
          Company,
          Company,
          function(x) + length(x) # decreasing order company
        ),
        fill= job_type
    )
  ) +
  geom_bar() +
  coord_flip() +
  labs(
    titel= "Aantal vacatures per bedrijf per funtietype",
    x= "Bedrijf"
    )
```

#### top 10 bedrijven - functie types

```{r}
# get top 10 companies not being consultancies and others alike
companies_of_interest <- c(
  "Bertelsmann SE & Co. KGaA - Corporate Center",
  "NN Group",
  "Rabobank",
  "belastingdienst",
  "Rijkswaterstaat",
  "Alliander",
  "MN",
  "ING",
  "Amarant",
  "ABN AMRO"
)

df_indeed_final %>%
  filter(Company %in% companies_of_interest) %>%
  ggplot(
    aes(
        x = reorder(
          Company,
          Company,
          function(x) + length(x) # decreasing order company
        ),
        fill= job_type
    )
  ) +
  geom_bar() +
  coord_flip() +
  labs(
    titel= "Aantal vacatures per bedrijf per funtietype",
    x= "Bedrijf"
    )
```

#### top 10 bedrijven - top 10 skills (gehele populatie)

```{r}


important_skills_full_pop <- table(df_indeed_skills$skills) %>% sort() %>% tail(10)

df_indeed_skills_date_filtered %>%
  filter(
    Company %in% companies_of_interest &
    skills %in% names(important_skills_full_pop)) %>%
  ggplot(
    aes(
        x = reorder(
          skills,
          skills,
          function(x) + length(x) # decreasing order company
        ),
        fill= Company
    )
  ) +
  geom_bar() +
  coord_flip() +
  labs(
    titel= "Aantal vacatures per bedrijf per funtietype",
    x= "Bedrijf"
    )
```

#### Top 10 bedrijven - top 10 skills

```{r}

df_important_companies <- df_indeed_skills_date_filtered %>%
  filter(Company %in% companies_of_interest)

important_skills <- table(df_important_companies$skills) %>%
  sort() %>%
  tail(10)

df_indeed_skills_date_filtered %>%
  filter(
    Company %in% companies_of_interest &
    skills %in% names(important_skills)) %>%
  ggplot(
    aes(
        x = reorder(
          skills,
          skills,
          function(x) + length(x) # decreasing order company
        ),
        fill= Company
    )
  ) +
  geom_bar() +
  coord_flip() +
  labs(
    titel= "Aantal vacatures per bedrijf per funtietype",
    x= "Bedrijf"
    )
```

Bovenstaande geeft handvatten om te bepalen welke skills momenteel
belangrijk zijn om te ontwikkelen onder consultants en welke
opdrachtgevers benaderd kunnen worden wegens het aantal vacatures. De
code en artefacten die dit document opleveren geven de mogelijkheid om
naar voorkeur de analyse aan te passen door bijvoorbeeld de focus van de
bedrijven te verleggen (indien grootste aanbieders van functies over de
tijd wijzigt).

### Ontwikkeling functie type en vraag opdrachtgevers {.tabset}

Ondanks dat het niet mogelijk is om nu nog een uitspraak te doen over de
ontwikkeling van vraag bij opdrachtgevers wordt er wel vast een artefact
aangeleverd in de vorm van een tijdserie op basis van functietype. Deze
grafiek en de bijbehorende code zijn hieronder en in de ondersteunende
scripts terug te vinden voor toekomstig gebruik. Omdat er nu geen
uitspraken worden gedaan i.v.m de statistische validiteit zal er geen
verdere toelichting bij de grafiek te vinden zijn.

Er is hier ook overweogen een tweede artefact mee te leveren, namelijk
een stuk code om het verloop van de vraag door individuele opdracht
gevers weer te geven. Echter zegt de grafiek door de geringe hoeveelheid
data te weinig. Om verkeerde interpetaties van de data te voorkomen
wordt deze code dan ook niet meegeleverd. Zodra er genoeg data verzamelt
is kan aan de hand van deze file en bijbehorende code een nieuw
onderdeel worden toegevoegd om deze verandering over de tijd accuraat
weer te geven.

#### tijdreeks - functie type

```{r}
df_indeed_final_date_filtered %>%
  group_by(
    listing_date,
    job_type) %>%
  summarise(
    req_per_day= n(),
    .groups= "keep") %>%
  ggplot(
    aes(
      x= listing_date,
      y= req_per_day
      )) +
  geom_line() +
  facet_wrap(
    vars(job_type),
    ncol=1) +
  stat_smooth(
    method = "lm",
    se= FALSE
    ) +
  scale_x_date(
    date_labels = "%a\n%d-%m",
    date_breaks = "week"
               )

knitr::knit_exit()
```

## Discussie

---
# Discussie
# 
#     Geef een kritische evaluatie van de resultaten (bv. beperkingen of verbeterpunten)
#     Sluit aan bij Deployment van CRISP-DM (tenzij dit echt wezenlijk deel van het project was, dan moet dat in 2. Methoden aan bod komen)
---

In dit hoofdstuk koment in iedergeval de kritische verbeterpunten en
tekortkoming aanbod. Ook wordt er aandacht besteed aan de deployement en
wat er daadwerkelijk van waarde wordt opgeleverd.

### Data omvang

Het eerste verbeterpunt zit in de omvang van de dataset. Het grootste
deel van dit onderzoek kon goed beantwoord worden binnen de tijd en met
de aanwezige middelen. Echter blijft een interessante deelvraag: de
ontwikkeling van de vraag. Grotendeels onbeantwoord. Inherent aan
tijdseries analyses is de lange termijn data die hiervoor nodig is. Bij
aanvang van dit project was dit al een duidelijke beperking. Echter was
de hoop er om met de hoeveelheid tijd die beschikbaar was toch een goede
tijdsreeks op te bouwen die inzichten kon verschaffen. Dit is achteraf
toch niet gelukt. Hoewel er in de basis genoeg data punten zijn om een
tijdsreeks analyse te kunnen doen, maken de gewenste groeperingen
(bedrijf, functie, skill) dat de dataset te ver versnipperd raakt. Dit
is vooral voor toekomstige project goed om rekening mee te houden

#### Data kwaliteit

In de methoden is de datakwaliteit al kort aanbod gekomen. Toch wordt
dit punt hier nog aangestipt. Door het ontbreken van een betrouwbare
API, de tijdspannen van dit onderzoek en de bewerkelijkheid van de data
zijn een aantal keuzes gemaakt die maken dat het eindproduct niet
volledig accuraat is. De belangrijkste is de analyse omtrent
functietypen. Deze is verdeeld in drie specialisaties en elke observatie
in de dataset is zo goed mogelijk in een van deze drie specialisaties
onderverdeeld. Echter, er zijn observaties waar eigenlijk geen van de
drie specialisaties bij passen (denk aan "data entry" functies). Ook
heeft er wegens tijdsgebrek maar een beperkte check plaatsgevonden op
deze categorisering en andere bewerkingen zoals het omrekening naar
maandsalaris of het extraheren het aantal jaren werkervaring. Dit zijn
kritische verbeterpunten. Bij het door ontwikkelen van de
functionaliteiten van dit project zou er daarom gekeken kunnen worden
naar:

-   Het uitbreiden van de functietypen

-   betere extractie van werkervaring

-   betere extractie van vaardigheden

-   validatie van de formule en de bepaling van salarissoorten (uur,
    maand, en jaar)

#### Data scraping

Als laatste zijn er twee kantekeningen bij het data scrapen. Allereerst
is alleen Indeed als bron gebruikt. Qua website omvang is dit misschien
een goede representatie maar het zou kunnen dat hier nog een beter
alternatief voor is. Het gevaar van dubbele vacatures binnenhalen bij
het gebruik van meerdere scrapers is dan natuurlijk een gevaar. Wegens
tijdsgebrek is hier geen verder onderzoek naar gedaan en staat dit nog
open voor verbetering.

De tweede kantekening betreft de zoek titel. De scraper en data is
gericht op een zo breed mogelijk profiel. Om deze reden is als
zoekopdracht "data" gebruikt. Echter een verbetering doe wordt
voorgesteld is de scraper om te bouwen naar een meer generieke scraper.
Hierbij kan de gebruikte zoek woorden vrij worden ingevuld en opgeslagen
in een variabele die ook wordt opgeslagen in de dataset. Voor dit
onderzoek zou dit hebben geholpen om accurater te zoeken naar analisten,
engineers, en scientisten. Dit sluit aan op de verdere kritiek onder
"data kwaliteit".

#### Deployment

Als laatste onderwerp het stuk deployment. Vanuit dit onderzoek worden
er een aantal artefacten opgeleverd. Deze zullen niet standaard in een
productie omgeving draaien maar kunnen wel voor ad-hoc analyses of door
ontwikkeling worden gebruikt.

De scripts zijn lost van dit notebook te draaien en geven House of Bèta
de mogelijkheid niet alleen het onderzoek te reproduceren maar ook
verder uit te breiden. In onderstaande tabel de artefacten die deel
uitmaken van dit onderzoek.

| Artefact                                         | Gebruik                                                                                                                                                                                                                                                                                          |
|---------------|---------------------------------------------------------|
| rmd_Capstone_DMAE_MaelvDijk.Rmd                  | Markdown document met de analyses om de vraag "Hoe positioneert HoB wat betreft het loon ten opzichten van de ontwikkeling van de vraag naar 'data specialisten'in Nederland zich kijkende naar de gevraagde vaardigheden, het geboden loon en mogelijke opdrachtgevers?" te kunnen beantwoorden |
| Data_cleaned / Data_raw                          | diverse .CSV bestanden die als brondata dienen voor het hier uitgevoerde onderzoek.                                                                                                                                                                                                              |
| indeed_scraper.R (Supporting_code)               | webscraper voor de Indeed vacature website. Te gebruiken om nieuwe data binnen te halen. de mogelijkheid is er om de functionaliteit uit te breiden met onder andere andere zoekwoorden.                                                                                                         |
| Scraped_indeed_data_wrangler.R (Supporting_code) | script om de gescrapte data (opgeslagen in .csv format) op te schonen en verijken met nieuwe datapunten. De mogelijkheid is er om de functionaliteit uit te breiden naar eigen inzien.                                                                                                           |
| Scraped_indeed_plotter.R (Supporting_code)       | script om de gescrapte data na schonen en verrijken op een uniforme wijze te plotten. Ook dit bestand kan naar eigen inzien uitgebreid worden.                                                                                                                                                   |

## Conclusie

---
# Conclusie en aanbevelingen
# 
#     Beantwoord de hoofdvraag
#     Geef eventuele aanbevelingen (SMART en terzake)
---

Het laatste onderdeel van dit onderzoek tracht de hoofdvraag: "Hoe
positioneert HoB wat betreft het loon ten opzichten van de ontwikkeling
van de vraag naar 'data specialisten'in Nederland zich kijkende naar de
gevraagde vaardigheden, het geboden loon en mogelijke opdrachtgevers?"
te beantwoorden.

Uit het onderzoek is gebleken dat HoB haar werknemers vanaf het 3e jaar
een competatief salaris lijkt te bieden. Hiermee zou de werknemer in het
3e jaar meer verdienen dan hij/zij in meer dan 50% van de gevallen had
kunnen verdienen afgaande op de vacatures op Indeed. In het eerste en
tweede jaar ligt het salaris boven het modale salaris op Indeed.

Wat betreft de vaardigheden die binnen HoB worden aangemoedigd om aan te
leren zit HoB in het segment waar de meeste vraag naar is (sql, R,
python, excel). Echter duikt hier ook een tekort op. Een groot deel van
dataprofessionals bezitten deze gevraagde skills of zijn deze aan het
ontwikkelen. Hoewel de vraag naar deze skills klaarblijkelijk nog steeds
aanwezig is zijn dit niet de plekken waar HoB zich op kan onderscheiden.
De vaardigheden waar HoB zich daadwerkelijk in zou kunnen onderscheiden
zijn: neural networks, ai, machine learning, azure en aws, sas, en big
data frameworks zoals spark en scala. Deze aanbeveling berust enerzijds
op het resultaat uit dit onderzoek zoals de meest gevraagde skills bij
de meest frequente opdrachtgevers, anderzijds komt dit vanuit
persoonlijke ervaring en gesprekken tijdens het uitvoeren van mijn eigen
opdrachten als data analist / data scientist.

het laatste onderdeel m.b.t de opdrachtgevers is voor nu niet te
beantwoorden door het tekort aan data. Wel kan HoB haar inspanningen
potentieel richten op de top 10 bedrijven (uitgezonderd andere
consultant bureaus) voor haar business development, op het moment van
schrijven zijn dat:

1.  Bertelsmann SE & Co

2.  NN Group

3.  Rabobank

4.  Belastingdienst

5.  Rijkswaterstaat

6.  Alliander

7.  MN

8.  ING

9.  Amarant

10. ABN AMRO

In het kort, HoB investeert in de ontwikkeling van haar personeel
waarbij het salaris in het 3e jaar een groot deel van de markt verslaat.
Er zijn voornamelijk kansen voor HoB om met haar specialisten verder te
richten op kennis van de cloud, big data oplossingen en diepgaandere
kennis over ai en machine learning

## oude code en tekst

------------------------------------------------------------------------

------------------------------------------------------------------------

------------------------------------------------------------------------

------------------------------------------------------------------------

------------------------------------------------------------------------

```{r Indeed salaris verdeling per skill, echo=FALSE}

df_indeed_rel_skill <- df_indeed_final[df_indeed_final$skill_count %in% c(3,4,5), ]

df_indeed_rel_skill %>%
    ggplot(aes(x= salary)) +
      geom_density(alpha=0.5) +
      geom_segment(
        aes(
          x =2763, xend =2763,
          y= 0.0000, yend=0.0001,
          linetype= "HoB 1ste jaar"
        )
      ) +
        geom_segment(
        aes(
          x =3087, xend =3087,
          y= 0.0000, yend=0.0001,
          linetype= "HoB 2de jaar"
        )
      ) +
        geom_segment(
        aes(
          x =3519, xend =3519,
          y= 0.0000, yend=0.0001,
          linetype= "HoB 3de jaar"
        )
      )+
      facet_wrap(vars(skill_count)) +
      labs(
        title="Verdeling geboden salaris per aantal gevraagde skills",
        y= "Frequentie (%)",
        x= "Geboden salaris (\u20ac)"
      ) +
      theme(
        plot.title=element_text(hjust=0.5))+
      scale_y_continuous(
        breaks= seq(0.0000, 0.0007, 0.0001),
        labels= seq(0.00, 0.07, 0.01)
      )
```

Hieronder wordt de data verkend. D001 is al een beetje geschoond en kan
hieronder makkelijk verkend worden. D002 bevat een ruwe data die niet
goed te gebruiken is. Hiervoor is het script
*'./supporting_code/Scraped_indeed_data_wrangler.R'* beschikbaar. Dit
script bevat functies om de data vanuit de scraper op te schonen zodat
er wel inzichten uit de data gehaald kunnen worden.

#### D001

Voor D001 wordt in dit onderdeel de verdeling bekeken voor het aantal
skills en salaris over de totale dataset en gesegmenteerd op baan type.

**Data Analist**

Wat gelijk opvalt aan deze dataset is dat de data analist de "minst
zware" functie lijkt. Er worden minder skills verwacht van een data
analist en het loon voor deze rol ligt ook doorgaans lager dan de andere
twee rollen.

**Data Engineer**

Het is gelijk duidelijk dat data engineers de minste vraag genieten maar
wel een zwaarder profiel moeten hebben. Het salaris van de data engineer
volgt ongeveer de verdeling van de data scientist. Wel valt op dat
ondanks de lagere hoeveelheid vacatures dat de data engineer het
zwaarste skill profiel dient te hebben.

**Data Scientist**

Naar data scientisten lijkt de hoogste vraag te zijn. Het skill profiel
van de data scientist is gemiddeld de verdeling neigt meer naar links
dan de data engineer. Ondanks dit lijkt de salaris verdeling gelijk te
zijn aan die verdeling van de data engineer.

**Andere inzichten**

Verder is in deze dataset te zien dat, wanneer er om meer skills
gevraagd wordt er een iets hoger salaris wordt geboden. Voor de hoogste
salaris groep lijkt dit echter niet te gelden. De aannamen (die nog
getoetst kan worden) is dat dit gaat om teamleiders of andere management
functies waar hard skills minder van belang zijn.

```{r D001 exploration, echo=FALSE}

kaggle_plots <- plot_kaggle_distribution(df_indeed_kaggle)


```
