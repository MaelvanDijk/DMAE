---
title: "Capstone_MaelvDijk_DMaE"
author: "Maël"
date: '2022-04-14'
output: 
  html_document:
    toc: true
    toc_float: true
    highlight: tango
    theme: journal
    code_folding: hide
    # number_sections: true
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE
  )

# make notebook output scrollable
options(width = 60)
local({
  hook_output <- knitr::knit_hooks$get('output')
  knitr::knit_hooks$set(output = function(x, options) {
    if (!is.null(options$max.height)) options$attr.output <- c(
      options$attr.output,
      sprintf('style="max-height: %s;"', options$max.height)
    )
    hook_output(x, options)
  })
})

library("tidyverse")
library("stringr") # textmining toepassingen
library("tseries") # tijdseries plotten
library("Hmisc") # basale data beschrijvingen


#custom functions
source('./supporting_code/Scraped_indeed_data_wrangler.R')
source('./supporting_code/Scraped_indeed_plotter.R')
source('./supporting_code/Kaggle_indeed_plotter.R')

#implement color palette for plots
HoB_color_palet <- c("#07E597", "#FE4F00", "#2C435E", "#2E7D7B", "#0B191D")

```

## Samenvatting

House of Bèta (HoB) blijkt vanaf haar derde jaar een competitief salaris
te bieden waarin het meer dan 50% van de markt verslaat. In de eerste
tweede jaren verdienen werknemers boven het modale loon. Hierin is zeker
te stellen dat HoB een competitief salaris biedt. Het blijkt echter dat
HoB nog ruimte heeft om de hard skills van haar personeel te
ontwikkelen. Het belangrijkste zijn skills m.b.t cloud, big data, en
geavanceerde data science skills. Deze skills worden steeds meer
gevraagd door de huidige opdrachtgevers en potentiële nieuwe
opdrachtgevers. Wat betreft nieuwe opdrachtgevers blijken er kansen te
liggen bij Bertelsmann en Amarant. Als laatste is het belangrijk om de
skills sql, python en R te blijven aanbieden en aansterken.

## Inleiding

Dit onderzoek richt zich op de vraag hoe competitief House of Bèta (HoB)
is op de markt voor data professionals. Hierin wordt gekeken naar zowel
het monetair salaris als de mogelijkheden voor HoB om een groter deel
van de markt te bedienen.

HoB heeft al langer de wens om inzichten te krijgen in de reden van
uitstroom van haar consultants, gezien dit een belangrijke asset is van
de organisatie. Wegens juridische en technische beperkingen is het
momenteel niet mogelijk hier een specifiek onderzoek naar te doen.

Deze wens is meer urgent geworden na de uitstroom die het moederbedrijf
van HoB, Talent&Pro, in 2021 heeft gezien. HoB verwacht dat deze
uitstroom op den duur ook binnen haar eigen consultants zal optreden.

Een tweede wens is om de markt beter te kunnen bedienen en als bedrijf
te groeien, zoals gepresenteerd in de doelstellingen van 2022. Deze
groei moet plaats vinden op het gebied van het aantal consultants,
opdrachten en de diversiteit van deze opdrachten. Door deze wensen en
doelstellingen te koppelen helpt dit onderzoek HoB om inzicht te krijgen
in een ontwikkelende markt, haar eigen monetaire arbeidsvoorwaarden op
waarde te schatten en biedt een basis voor verdere analyse aangaande de
uitstroom van haar personeel.

### Businessvraag

De wensen van HoB worden met de volgende onderzoeksvraag onderzocht:
"Hoe positioneert HoB zich ten opzichte van andere werkgevers op de
Nederlandse arbeidsmarkt van data professionals kijkend naar loon,
skills en potentiële opdrachtgevers?"

Deze vraag kan verder worden opgedeeld in drie deelvragen:

1\. Hoe verhoudt het salaris van HoB zich tot andere werkgevers op de
Nederlandse arbeidsmarkt van data professionals?

2\. In hoeverre sluit het cursus,- en opleidingsaanbod van HoB zich aan
op de vraag van andere werkgevers op de Nederlandse arbeidsmarkt van
data professionals?

3\. Welke werkgevers die actief zijn op de Nederlandse arbeidsmarkt van
data professionals worden nog niet bediend door HoB?

### Data mining goals

Om tot de businessdoelen te komen zijn er een aantal data mining doelen
opgesteld. Gezien HoB nog geen datamining activiteiten uitvoert betekent
dit dat een groot deel van dit onderzoek besteed wordt aan ETL.

Allereerst moet er een dataset opgehaald worden met een corpus van
skills. Vervolgens moeten vacatures gescraped worden van een
representatieve vacaturebank. Daarna moet er textmining plaatsvinden om
tot tabulaire data te komen en deze data te verrijken. Als laatste
moeten er analyses plaats vinden om de vraag van de business te kunnen
beantwoorden.

### Business Success criteria

De volgende kwalitatieve eisen zijn door HoB gesteld: er moet een
analyse komen van HoB versus marktinkomen, en een verdeling van de vraag
(naar skills) onder bedrijven.

Daarnaast wordt er een bruikbare scraper en andere data analyse
artefacten voor vervolg onderzoek opgeleverd.

## Methode

Dit hoofdstuk behandelt de data en het gebruik. In "data understanding"
worden kenmerken van de dataset besproken, "data preperation en
modelling" gaat in op het schonen en verrijken van de data. Als laatste
gaat "reproduceren van het onderzoek" over de reproduceerbaarheid van
dit onderzoek.

### Data understanding

Hieronder worden de scope, de gebruikte datasets en kenmerken
beschreven.

#### Gekozen datasets en scope

Om te beginnen is hieronder een datalog weergegeven. Dit bevat
informatie over de datasets in de ruwe vorm (te vinden in
".\\Data_raw")".

| ID   | dataset name                   | format | beschrijving                                                                         | dataset owner | Locatie                                                                                         |
|------------|------------|------------|------------|------------|------------|
| D001 | indeed_job_dataset.csv         | CSV    | dataset met job post data vanuit de VS voor data anlysten, scientisten en engineers. | Elroy         | extern: <https://www.kaggle.com/datasets/elroyggj/indeed-dataset-data-scientistanalystengineer> |
| D002 | <DATE> scraped indeed data.csv | CSV    | scraped data voor functies met het woord "data" in nederland                         | Maël          | scraper: ./Supporting_code/indeed_scraper.R                                                     |

: datalog (shortened)

D001 is gekozen als basis (zie ook hoofdstuk "Addendum" voor
aansluitende analyses). Hierin staan onder andere skills en
functietypen. Dit legt een goede basis voor dit onderzoek, de
functietypen sluiten namelijk aan op de specialisaties binnen HoB.
Andere datasets hadden te weinig observaties of misten cruciale
gegevens, vandaar de keuze voor D001.

D002 is gescrapte data van Indeed. De rationale achter het scrapen van
Indeed komt uit het tekortschieten van bestaande API's en afwezigheid
van relevante datasets. Indeed is gekozen omdat dit een van de grotere
vacaturebanken is. Ook kan de scraper andere vacatures binnen halen,
niet gerelateerd aan het data domein, wat deze multipurpose maakt.

#### Beschrijving van de ruwe data

Beginnend bij de beschrijving van D001. Deze dataset is al erg
opgeschoond. De basale kenmerken zijn in onderstaande overzicht te
vinden. Voor dit onderzoek lijken gelijk twee kolommen interessant. Dit
zijn "Skill" en "Job_Type". Hierin staan respectievelijk een lijst van
(hard)skills en de functietypen per observatie.

```{r basic D001 data, echo=FALSE, max.height='400px', comment= NA}
# kenmerken D001 dataset
df_indeed_kaggle <-read.csv(".\\Data_raw\\D001\\indeed_job_dataset.csv")

describe(df_indeed_kaggle)
```

het overzicht hierboven toont onder andere dat het aantal unieke
"Skills" bijna gelijk is aan het aantal unieke rijen. Hier zit echter
een fout. Het is namelijk zo dat [SQL, Python] en [Python, SQL]
hetzelfde betekenen maar wordt gezien als twee unieke rijen. Daarnaast
is het opvallend dat de verdeling tussen data analist (31%), data
engineer (24%) en data scientist (45%) niet evenredig is.

Dataset D002 bevat Indeed data. Hiervoor is er gescraped naar functies
met het woord "data". D002 bestaat uit meerdere losse bestanden (één
voor elke dag dat er een run is geweest). Hieronder de voornaamste
kenmerken van de run van 2022-03-08:

```{r basic D002 data, echo=FALSE, max.height='400px', comment= NA}
# kenmerken D002 dataset
df_indeed_scraped <- read.csv(".\\Data_raw\\D002\\2022-03-08 scraped indeed data.csv")

describe(df_indeed_scraped)
```

Afgaande op de kenmerken wordt duidelijk dat:

1.  De dataset heeft als vroegste datum 6 februari 2022
2.  Er zijn geen volledige dubbele rijen.
3.  283 rijen unieke URL's en 266 rijen unieke beschrijvingen bevatten.
    Dit duidt op dubbele vacatures.
4.  Er zijn 226 bedrijven in de dataset aanwezig.
5.  Het salaris lijkt foute symbolen te bevatten en lijkt zowel uur-,
    maand-, en jaarsalarissen te bevatten.
6.  Een vacature lijkt gemiddeld 22 dagen online te staan met een
    mediaan van 30 dagen. Wat aangeeft dat er relatief weinig nieuwe
    vacatures op de dag van scrapen zijn geplaatst.

Uiteindelijk lijkt de dataset vrij goed geconstrueerd op de kolommen met
skills en salaris na. Ook mist er informatie over functietypen.

### Data preperation en modeling

Onderstaande processflow geeft in grote lijnen het proces van ruwe data
tot resultaat weer.

Het volledige proces bestaat uit drie segmenten. Bovenin betreft D001.
Segment twee (midden) betreft D002. Onderin staat segment drie waarin
het eindresultaat weergegeven wordt.

![Data process flow](Images/data%20flow%20process.png)

Omdat de tijd beperkt is voor dit onderzoek zijn er een aantal aannames
gedaan om problemen te voorkomen en de complexiteit te verkleinen,
verder wegens beperkte tijd en de afwezigheid van een ML model is er
voor gekozen om de missende waarden niet onderhanden te nemen:

| Process stap        | Betreffende kolom | Versimpelde weergaven                                                                                                                                                                                                                |
|------------------|------------------|-------------------------------------|
| Scraping            | Salary            | Er is bij weergaven salarissen altijd voor het laagste salaris gegaan                                                                                                                                                                |
| ETL (midden-rechts) | job_type          | Er zijn drie type (analist, engineer, scientist). De dataset wordt verrijkt met een van deze type zelfs al wijkt dit af. Zo kan de functie "data entry" als "data analist" worden geclassificeerd.                                   |
| ETL (midden-rechts) | salary            | Salaris \> 10.000 is jaarsalaris of salaris \< 100 is uur tarief. Dit wordt omgerekend naar maandsalaris (delen door 12 of maal 168 respectievelijk). Alles tussen de 100 en 1000 wordt als scraping fout gezien en omgezet naar NA. |

: Aannamens tijdens process stappen,

De scripts voor deze stappen zijn te vinden in "./Supporting_code":

| Process stap        | Script naam                    |
|---------------------|--------------------------------|
| ETL (linksboven)    | Generate_ref_skill_list.R      |
| Scraping            | indeed_scraper.R               |
| ETL (midden-rechts) | Scraped_indeed_data_wrangler.R |

: process stappen en bijbehorende scripts.

De scripts en betreffende functies zorgen voor een dataset die gebruikt
kan worden voor dit onderzoek. De scripts zijn voorzien van helpende
comments om het gebruik te vergemakkelijken.

### Reproduceren van het onderzoek

De mappen: "Data_cleaned", "Data_raw", en "Supporting_code" bevatten
alle data en code die nodig is om dit onderzoek te reproduceren. In dit
markdown document worden de data en benodigde scripts aangeroepen om tot
het eindresultaat te komen. De benodigde R libraries zijn: "tidyverse",
"stringr", "lubridate", "tibble", "textTinyR", "XML", "rvest",
"tseries".

Verder is aan het eind van dit document een hoofdstuk "Addendum"
toegevoegd. Dit zijn stukken code en analyses geschreven ten behoeven
van het onderzoek maar wegens ruimte gebrek of de statistische
betrouwbaarheid achterwegen gelaten.

## Resultaten

Dit hoofdstuk beantwoord de deelvragen aan de hand van diverse analyses.
Ook wordt besproken welke vragen (nog) niet kunnen worden beantwoord en
welke artefacten worden aangeleverd.

```{r clean and enrich D002, echo=FALSE}
# clean en verrijk indeed data met custom functies
df_indeed_merged <- merge_scraped_data()
df_indeed_cleaned <- clean_scraped_date(df_indeed_merged)

# geschoonde en verrijkte dataset
df_indeed_final <- enrich_scraped_date(df_indeed_cleaned)

# dataset voor skill analyse
df_indeed_skills <- unnest_skills(df_indeed_final)

# dataset voor tijdserie analyse
df_indeed_final_date_filtered <- filter_dates(df_indeed_final)
df_indeed_skills_date_filtered<- filter_dates(df_indeed_skills)

rm(
  df_indeed_kaggle,
  df_indeed_scraped,
  df_indeed_merged,
  df_indeed_cleaned
)
```

### Hoe verhoudt het salaris van HoB zich tot andere werkgevers op de Nederlandse arbeidsmarkt van data professionals? {.tabset}

HoB heeft op haar website aangegeven wat een collega kan verdienen in
jaar een, twee en drie (€2.763,- , €3.087,- , €3.591,- respectievelijk).
Om deze deelvraag te beantwoorden worden deze drie salarissen gebruikt.

De grafiek hieronder geeft de salaris verdeling van data vacatures op
Indeed weer. Het eerste wat duidelijk wordt is dat het loon bij HoB
boven modaal ligt. Daarnaast is het salaris in jaar drie hoger dan 50%
van de salarissen geboden op Indeed.

```{r plot_hob_salary_vs_indeed_salary_dist, echo=FALSE}
plot_hob_salary_vs_indeed_salary_dist(
  df_indeed_final,
  first_salary= 2763,
  second_salary= 3087,
  third_salary= 3519
)
```

Bovenstaande analyse zegt echter niet alles. Hierin worden drie
verschillende maandsalarissen van HoB vergelijken met de gehele
onderzoekspopulatie.

Hieronder is de salaris verdeling te zien gesegmenteerd op gevraagde
ervaring in de vacatures. Door dataset D002 (1614 observaties) op te
delen in negen segmenten op basis van gevraagde jaren werkervaring is de
verdeling gevoeliger en minder accuraat. Toch geeft het de indruk dat
het salaris geboden door HoB doorgaans beter is dan het salaris op
Indeed.

```{r plt_hob_vs_indeed_salary_dist_per_experience, echo=FALSE}

plt_hob_vs_indeed_salary_dist_per_experience(
  df_indeed_final,
  first_salary= 2763,
  second_salary= 3087,
  third_salary= 3519
)
```

Het salaris bij HoB wordt niet alleen bepaalt door werkervaring. Ook het
afronden van cursussen en het opdoen van skills wordt beloond. Wat
betreft hard skills kan er vanuit worden gegaan dat een consultant vier
tot zes skills bezit na de eerste drie jaar bij HoB. Dit zijn data
modeling, excel, R, Python, SQL en PowerBI.

In onderstaande drie grafieken is het mediaan salaris op Indeed
weergegeven verdeeld over het gevraagde aantal skills. De zwarte lijn
geeft het bijbehorende HoB salaris aan. De balken kleuren groen op het
moment dat het salaris van HoB hier boven ligt.

Vanaf het derde jaar blijkt HoB echt een competitief salaris te bieden.
Hierbij kan verwacht worden dat een consultant zo'n vier tot zes skills
bezit. Daarmee verslaat het de markt kijkend naar skill,-, loonratio.

#### 1ste jaars HoB

```{r}
plot_indeed_median_salary_per_amount_skills(
    df_indeed_final,
    add_ref_line= TRUE,
    ref_salary= 2763
)
```

#### 2de jaars HoB

```{r}
plot_indeed_median_salary_per_amount_skills(
    df_indeed_final,
    add_ref_line= TRUE,
    ref_salary= 3087
)
```

#### 3de jaars HoB

```{r}
plot_indeed_median_salary_per_amount_skills(
    df_indeed_final,
    add_ref_line= TRUE,
    ref_salary= 3519
)

```

###  {.unlisted .unnumbered}

Bovenstaande analyse zou nog verdiept kunnen worden in een vervolg
onderzoek door het gevraagde aantal jaren werkervaring mee te nemen als
extra dimensie. Dit levert nu echter te weinig observaties (11) per
categorie op.

### In hoeverre sluit het cursus,- en opleidingsaanbod van HoB zich aan op de vraag van andere werkgevers op de Nederlandse arbeidsmarkt van data professionals? {.tabset}

Deze deelvraag behoeft twee analyses. Enerzijds is het van belang om te
weten hoe het huidige aanbod van HoB aansluit op de huidige vraag.
Daarnaast is het van belang om te weten hoe de vraag naar Skills zich
ontwikkelt. Door gebrek aan data is dit laatste lastig te analyseren
toch wordt hiertoe een poging gedaan.

Allereerst, de huidige vraag naar skills. Hieronder zijn twee grafieken
te vinden. De eerste grafiek geeft de top 20 skills weer, de tweede
grafiek de minst gevraagde 20.

#### top skills

```{r}

# get sorted vector of all skills
all_skills <- df_indeed_skills$skills %>% sort(decreasing= TRUE)

# create a top and bottom 20
top_skills <- table(df_indeed_skills$skills) %>% sort() %>% tail(20)
bottom_skills <- table(df_indeed_skills$skills) %>% sort() %>% head(20)

# plot top skills
df_indeed_skills %>%
    # filter df based on most sought skill
  filter(skills %in% names(top_skills)) %>% 
  ggplot(
      aes(
        x = reorder(
          skills,
          skills,
          function(x) + length(x) # decreasing order skill
          ),
          fill= job_type
          )
      ) +
  geom_bar() +
  theme(
    axis.text.y = element_text(
      face= 'bold',
      size= 14)) + # Rotate x-labels and change font
      coord_flip() +
  labs(
    title= "Functie verdeling over 20 meest gevraagde skills",
    y= "aantal"
  )+
  theme_classic()+
  theme(axis.title.y=element_blank())+
  scale_fill_manual(values=HoB_color_palet)


```

#### Bottom skills

```{r}

# plot bottom skills
df_indeed_skills %>%
    # filter df based on least sought skill
  filter(skills %in% names(bottom_skills)) %>% 
  ggplot(
      aes(
        x = reorder(
          skills,
          skills,
          function(x) + length(x) # decreasing order skill
          ),
          fill= job_type
          )
      ) +
  geom_bar() +
  theme(
    axis.text.y = element_text(
      face= 'bold',
      size= 14)) + # Rotate x-labels and change font+
  scale_y_continuous(
      breaks= seq(0, 3, 1),
      labels=seq(0, 3, 1),
      limits= c(0,3)
    )+
      coord_flip() +
  labs(
    title= "Functie verdeling over 20 minst gevraagde skills",
    y= "aantal"
  )+
  theme_classic()+
  theme(axis.title.y=element_blank())+
  scale_fill_manual(values=HoB_color_palet)
```

###  {.unlisted .unnumbered}

Gezien bij HoB momenteel de nadruk wordt gelegd op Python, SQL, R, SAS,
PowerBi en cloud (AWS, Azure) technologie, sluit dit goed aan op de
huidige vraag binnen de markt. Een gat blijkt te zitten in de specifieke
vaardigheid "Tableau" waar geen cursus voor wordt geboden door HoB en de
afwezigheid van opleidingen in de richting van big data en specifieke
data science skills (spark, ai, (neural) networks, etc.)

Gezien er geen overlap zit tussen de opleidingen die HoB biedt en de
minst gevraagde skills lijkt er momenteel geen reden te zijn om met
bepaalde opleidingen te stoppen.

Interessant om nog op te merken: SQL en Python beslaan 24% van de totale
vacatures, wordt R ook meegenomen dan is dit 32%. Een focus op alleen
dit deel van de skills lijkt een goed deel van de markt te dekken. Dit
zijn dus absoluut skills waar de focus op moet blijven liggen.

#### Ontwikkeling skillbehoeften

Met de huidige ontwikkelingen op het gebied van big data en cloud
technologie die in de praktijk zichtbaar zijn is het aannemelijk dat het
merendeel van deze top 20 alleen maar belangrijker gaan worden.

Om een sterke trend analyse te doen is minstens \~3 jaar aan data nodig.
Dit is een beperking die verder in het hoofdstuk "discussie" wordt
besproken.

Hieronder zijn zes skills gekozen die in iedergeval in de gaten gehouden
kunnen worden. Momenteel vertellen de grafieken dat de vraag doorgaans
dalende is (groene trendlijn) echter is de aannamen dat na het
verzamelen van meer data een duidelijke opwaartse trend zichtbaar zal
worden. Deze aannamen is op basis van ervaring binnen de consultancy
waar minder opdrachten beschikbaar zijn, vaak als gevolg van nieuwe
budgetering en invulling van deze budgetten door organisaties.

Mocht de behoeften aanwezig zijn om andere skills te onderzoeken dan kan
de bijbehorende code makkelijk worden aangepast.

###  {.tabset .unlisted .unnumbered}

#### SQL

```{r SQL instroom}
plot_indeed_timeseries_data(df_indeed_skills_date_filtered, "sql")
```

#### Python

```{r Python instroom}
plot_indeed_timeseries_data(df_indeed_skills_date_filtered, "python")
```

#### R

```{r R instroom}
plot_indeed_timeseries_data(df_indeed_skills_date_filtered, "r")

```

#### Tableau

```{r Tableau instroom}
plot_indeed_timeseries_data(df_indeed_skills_date_filtered, "tableau")
```

#### Azure

```{r Azure instroom}
plot_indeed_timeseries_data(df_indeed_skills_date_filtered, "azure")
```

#### AWS

```{r AWS instroom}
plot_indeed_timeseries_data(df_indeed_skills_date_filtered, "aws")
```

### Welke werkgevers die actief zijn op de Nederlandse arbeidsmarkt van data professionals worden nog niet bediend door HoB? {.tabset}

De laatste deelvraag binnen dit onderzoek richt zich op de mogelijke
opdrachtgevers. Hiervan was het oorspronkelijk de bedoeling om een trend
analyse te doen per bedrijf. Echter is dit door het tekort aan data niet
te realiseren. In plaats daarvan wordt hier gekeken naar de huidige gap
tussen de organisaties die vacatures uit hebben staan op Indeed en de
organisaties die HoB bedient. Wel is in de bijlagen "Addendum" een stuk
over de trend van de algehele markt te vinden.

In de grafiek "top 20 bedrijven - functie types" hieronder wordt de top
25 bedrijven getoond op basis van het aantal geplaatste vacatures.
Grofweg 50% van deze organisaties betreffen detacheerders (waaronder
House of Bèta). Dit is een interessant gegeven, de aannamen hier is
namelijk dat de reden dat er zoveel vraag vanuit detacheerders en
consultants is komt door de grote vraag naar data professionals in het
algemeen.

Buiten dat de data lijkt uit te wijzen dat er grote vraag naar data
professionals is, is het vooral interessant om te kijken naar de
organisaties met de hoogste vraag die niet vallen onder consultancy of
detacheerders. In de grafiek "top 10 bedrijven (excl. detachering) -
functie types" is een subset gemaakt van deze 25 bedrijven zonder
consultancies of detacheerders. Uit deze grafiek zijn op dit moment twee
partijen waar HoB geen personeel aan levert: Bertelsmann SE & Co, en
Amarant.

Uiteraard is dit een summiere analyse om een daadwerkelijke gap aan te
tonen vandaar nog twee aanvullende analyses. De grafiek "top 10
bedrijven - top 10 skills (gehele populatie)" laat zien dat voornamelijk
python, sql, R, ai en (neural) network skills veel gezocht wordt door de
subset van bedrijven. De grafiek "Top 10 bedrijven - top 10 skills
(binnen top 10 bedrijven)" laat eenzelfde verdeling zien maar dan vanuit
een andere invalshoek. De eerste van deze twee grafieken beschouwt de
meest gevraagde skills vanuit de hele onderzoeks populatie. De tweede
grafiek daarentegen laat toont alleen de top skills die daadwerkelijk
door deze bedrijven worden gezocht. Uit deze laatste grafiek komen drie
inzichten naar voren. De belangrijkste skills blijven ongewijzigd, SAS
(HoB focust hier sinds kort op) komt naar voren op de 5e plek. De
overige vijf skills hebben veelal te maken met big data toepassingen.

#### top 25 bedrijven - functie types

```{r}
top_companies <- table(df_indeed_final$Company) %>%
  sort() %>%
  tail(25) %>%
  names()

plot_job_type_per_company(df_indeed_final, top_companies)
```

#### top 10 bedrijven (excl. detachering) - functie types

```{r}
# get top 10 companies not being consultancies and others alike
companies_of_interest <- c(
  "Bertelsmann SE & Co. KGaA - Corporate Center",
  "NN Group",
  "Rabobank",
  "belastingdienst",
  "Rijkswaterstaat",
  "Alliander",
  "MN",
  "ING",
  "Amarant",
  "ABN AMRO"
)

plot_job_type_per_company(df_indeed_final, companies_of_interest)
```

#### top 10 bedrijven - top 10 skills (gehele populatie)

```{r}


important_skills_full_pop <- table(df_indeed_skills$skills) %>%
  sort() %>%
  tail(10) %>%
  names()

plot_skills_per_company(
  df_indeed_skills_date_filtered,
  companies_of_interest,
  important_skills_full_pop
)
```

#### Top 10 bedrijven - top 10 skills (binnen top 10 bedrijven)

```{r}
df_important_companies <- df_indeed_skills_date_filtered %>%
  filter(Company %in% companies_of_interest) 

important_skills <- table(df_important_companies$skills) %>%
  sort() %>%
  tail(10) %>%
  names()

plot_skills_per_company(
  df_indeed_skills_date_filtered,
  companies_of_interest,
  important_skills
)
```

###  {.unlisted .unnumbered}

Een kanttekening: Bertelsmann SE & Co. lijkt de uitslag van de top 10
analyses te beïnvloeden door hun sterke aanwezigheid. NN Group heeft
deze invloed blijkbaar minder ondanks het hoger aantal geplaatste
vacatures. Dit lijkt te komen omdat NN in al haar vacatures minder vaak
skills benoemd (totaal 152) dan Bertelsmann (totaal 388).

Ook inzicht in de concurrentie kan waardevol zijn, dit kan namelijk de
vraag naar vaardigheden weergeven van organisaties die openstaan voor
consultancy of detachering. De aannamen is dat andere detacheerders
vragen naar skills die hun opdrachtgevers zoeken, nu en toekomstig. In
de grafiek hieronder zijn de meest gevraagde skills onder de top 10
detacheerders getoond. Hieruit lijken drie nieuwe skills naar voren te
komen, namelijk: Stata, Matlab en Gis. Omdat niet veel partijen hier op
in zetten lijkt dit niet een belangrijke set skills voor algehele
marktbediening.

#### Top 10 skills per detacheerder

```{r}
deta_of_interest <- c(
  "YoungCapital",
  "Createment",
  "Breinstein Detachering",
  "Capgemini",
  "Page Personnel",
  "newmonday",
  "House of Bèta",
  "CarreerValue",
  "Good Company",
  "Mploy Associates"
)

df_important_deta <- df_indeed_skills_date_filtered %>%
  filter(Company %in% deta_of_interest) 

important_skills_deta <- table(df_important_deta$skills) %>%
  sort() %>%
  tail(10) %>%
  names()

plot_skills_per_company(
  df_indeed_skills,
  deta_of_interest,
  important_skills_deta
  )
```

Om deze deelvraag kort samen te vatten: HoB heeft een kans om bij
Bertelsmann SE &Co. en Amarant nieuwe opdrachten binnen te halen, mits
zij openstaan voor detachering en/of consultancy. Om haar positie bij
partijen te versterken is het naast bestaande cursussen nog waardevol om
te kijken naar big data en gevorderde data science skills.

## Discussie

In dit hoofdstuk komen de verbeterpunten en tekortkoming aanbod. Ook
wordt er aandacht besteed aan de deployement en daadwerkelijke
deliverables.

### Data omvang

Allereerst de omvang van de dataset. Een groot deel van dit onderzoek is
beantwoord met de beschikbare data. Echter zijn verdiepingen op trend
analyses achterwegen gelaten alsmede analyses over meerdere dimensies
waarvoor de dataset te klein is. Bij aanvang van dit project was de
inschatting van de hoeveelheid data die binnengehaald kon worden niet
direct duidelijk. Het voornaamste verbeterpunt is om de dataset uit te
breiden voor verdere analyses. Een leerpunt is dat analyses over drie
dimensies een dataset erg zullen versnipperen, hier moet volgende keer
rekening mee gehouden worden bij het beoordelen van het aantal nodige
observaties.

### Data kwaliteit

Datakwaliteit is al kort aangestipt door het project heen. Het ontbreken
van een API en de tijdsdruk voor dit onderzoek heeft betekend dat er
keuzes zijn gemaakt ik en het scrapen, schonen en verrijken van de data.
De belangrijkste om hier te benoemen zijn: functie type en skills.

Allereerst zijn er fouten aanwezig in de functie type. Er is onderscheid
gemaakt tussen data analist, data engineers, en data scientisten. Hoewel
er vacatures kunnen zijn voor hele andere data functies is iedere
functie geforceerd in een van deze catagorieën.

De tweede kwaliteit issue zit in de skills. Een goed voorbeeld is de
vaardigheid "network". Dit had waarschijnlijk "neural network" moeten
zijn. Wanneer er gezocht wordt op "power bi" zal "powerbi" niet worden
gevonden hierdoor mist er data. Ook zijn skills zoals "sql" in veel
verschillende varianten te vinden zoals "mysql". Deze skills zijn apart
opgeslagen, dit geeft een vertekende weergaven van de vraag naar de
echte skills.

### Data scraping

Als laatste zijn er twee kanttekeningen bij het data scrapen. Allereerst
is alleen Indeed als bron gebruikt. Qua website omvang is dit misschien
een goede representatie maar het zou kunnen dat hier nog een beter
alternatief voor is. Het gevaar van dubbele vacatures binnenhalen bij
het gebruik van meerdere scrapers is dan natuurlijk een gevaar. Wegens
tijdsgebrek is hier geen verder onderzoek.

Het tweede punt betreft de query. De scraper is gericht op een zo breed
mogelijk profiel. Om deze reden is als zoekopdracht "data" gebruikt. Een
voorgestelde verbetering is de scraper om te bouwen naar een generieke
scraper. Hierbij kan de query vrij worden ingevuld en opgeslagen in een
variabele die ook wordt opgeslagen in de dataset. Voor dit onderzoek zou
dit hebben geholpen om accurater te zoeken naar analisten, engineers, en
scientisten. Dit sluit aan op de verdere kritiek onder "data kwaliteit".

### Deployment

Als laatste deployement. Bij dit onderzoek worden een aantal artefacten
aangeleverd die gebruikt kunnen worden voor ad-hoc analyses. De scripts
en code zijn zonder de markdown te gebruiken en makkelijk te bewerken.
De code is voorzien van commentaar en kan relatief makelijk worden
uitgebreid met nieuwe functionaliteiten waar nodig.

Gezien HoB haar IT landschap aankoopt en niet zelf ontwikkelt is de
verwachting dat de artefacten hier aangeleverd niet standaard draaien in
een productie omgeving. Waarschijnlijker is dat de artefacten voor
ad-hoc analyses toegepast kan worden voor verder onderzoek wanneer
nodig. Tot die tijd zal de code te vinden zijn in een publieke repo op
GitHub: <https://github.com/MaelvanDijk/DMAE>.

## Conclusie

Hoe positioneert HoB zich ten opzichte van andere werkgevers op de
Nederlandse arbeidsmarkt van data professionals kijkend naar loon,
skills en potentiële opdrachtgevers?

Het is duidelijk geworden data HoB een competatief salaris biedt aan
haar medewerkers, In iedergeval vanaf jaar drie waar zij meer dan 50%
van de vacatures in loon voorbijgaan. Voor zowel jaar een, twee, en drie
verdient een HoB consultant boven modaal. Kijkende naar de gehele
onderzoeks populatie. Hierin lijkt er, afgaande op het markt aanbod van
monetair salaris, niet direct een aanleiding te zijn voor uitstroom bij
HoB. Hierbij moet wel gesteld worden dat aanbiedingen van opdrachtgevers
om collega's direct over te nemen na hun opdracht niet meetbaar zijn en
daarbij kunnen afwijken van de aanbiedingen zichtbaar op Indeed.

Op het gebied van skills en opdrachtgevers doet HoB het goed. Zij
bedient een groot deel van de bedrijven die de meeste vacatures uit
hebben staan. Hierin bestaat voor HoB de kans om met Amarant en
Bertelsmann zaken te gaan doen. Wat betreft de skills is het belangrijk
dat HoB blijft inzetten op de belangrijkste skills (sql, python en r)
maar ook de verdieping aanbiedt op het gebied ai, neuralenetwerken en
big data oplossingen (scala, spark en hive).

Kortom HoB lijkt competitief zowel op loon als aanbod waarbij kansen
bestaan om vooral naar opdrachtgevers toe verder uit te breiden in zowel
skills als partners.

Om tot deze conclusie te komen zijn naast de analyses in het
hoofddocument nog andere analyses gedaan deze zijn te vinden in de
bijlagen "Addendum".

## Addendum

In deze bijlagen worden nog een paar analyses gedeeld die geen onderdeel
meer uitmaken van de hoofdtekst. De overwegingen hiervoor zijn ruimte
gebrek, data gebrek en een lage statische betrouwbaarheid. Toch worden
de code, grafieken en analyses hieronder nog meegenomen ter referentie
en om de mogelijkheid te bieden om deze later toe te passen. De
grafieken zijn verder niet voorzien van opmaak in tegenstelling tot de
grafieken in de hoofdtekst gezien deze dienen voor exploratie doeleinden
en zijn niet bedoelt voor presentaties o.i.d.

Analyse D001

D001 is weinig behandelt in de hoofdtekst omdat de focus van dit
onderzoek de Nederlandse banenmarkt heeft. Toch bevat D001 interessante
inzichten. Hoewel de vertaling naar de Nederlandse markt niet makkelijk
te maken is. Zo is loon nagenoeg niet vergelijkbaar (bijv. door
afwezigheid van medische voorzieningen in het Amerikaanse salaris). Toch
kan kunnen overkoepelende inzichten verhelderend werken

Neem de eerste van onderstaande drie grafieken. Hierin kan uit de
verdeling worden afgeleid dat een data analist een doorgaans lichter
profiel (minder skills) nodig heeft dan een data engineer of data
scientist. Wanneer vervolgens de tweede grafiek beoordeeld wordt komt er
nog iets interessants naar boven. Het feit dat data scientisten meer
verdienen dan data engineers. Gezien het grote gat tussen de vraag naar
data engineers en data scientisten in deze set is dit misschien niet
geheel onverwacht. Maar, een hypothese bij aanvang van dit onderzoek was
dat data engineers het hoogste salaris zouden genieten van deze drie
rollen.

een laatste interessante ontdekking is dat het aantal gevraagde skills
nauwelijks toeneemt naarmate het aangeboden salaris stijgt. De curve in
de derde grafiek verschuift nagenoeg niet naar rechts naarmaten er in
hogere salaris groepen wordt gekeken. Hier zou een aannamen zijn dat
werknemers in het data veld worden beloond op basis van diepgaande
kennis en specialisatie in plaats van een breed en oppervlakkig kennis
gebied. Het kantel punt lijkt hier te liggen op ongeveer zeven á acht
skills.

```{r D001 exploration, echo=FALSE}

df_indeed_kaggle <- read.csv(".\\Data_raw\\D001\\indeed_job_dataset.csv")

kaggle_plots <- plot_kaggle_distribution(df_indeed_kaggle)

rm(kaggle_plots)

```

### Ontwikkeling functie type opdrachtgevers {.tabset}

Ondanks dat het niet mogelijk is om nu nog een uitspraak te doen over de
ontwikkeling van vraag bij opdrachtgevers wordt er wel vast een artefact
aangeleverd in de vorm van een tijdserie op basis van functietype. Deze
grafiek en de bijbehorende code zijn hieronder terug te vinden voor
toekomstig gebruik. Omdat er te weinig data is om een goede tijdsreeks
analyse te maken zal de grafiek verder niet van commentaar worden
voorzien.

Er is hier wel overwogen een tweede artefact mee te leveren, namelijk
een stuk code om het verloop van de vraag door individuele opdracht
gevers weer te geven. Echter zegt de grafiek door de geringe hoeveelheid
data te weinig. Om verkeerde interpetaties van de data te voorkomen
wordt deze code dan ook niet meegeleverd. Zodra er genoeg data verzamelt
is kan aan de hand van deze file een nieuw onderdeel worden toegevoegd
om deze verandering over de tijd weer te geven.

```{r}
df_indeed_final_date_filtered %>%
  group_by(
    listing_date,
    job_type) %>%
  summarise(
    req_per_day= n(),
    .groups= "keep") %>%
  ggplot(
    aes(
      x= listing_date,
      y= req_per_day
      )) +
  geom_line() +
  facet_wrap(
    vars(job_type),
    ncol=1) +
  stat_smooth(
    method = "lm",
    se= FALSE
    ) +
  scale_x_date(
    date_labels = "%a\n%d-%m",
    date_breaks = "week"
               ) +
  labs(
    title= "Ontwikkeling vraag per functie type"
  )


```

### Ontwikkeling vraag naar data professionals

Voor de deelvraag "In hoeverre sluit het "curriculum" van HoB zich aan
bij de ontwikkelende markt?" was het naast de huidige situatie ook de
bedoeling een trend analyse te doen. Helaas is duidelijk geworden dat er
te weinig data beschikbaar is om een betrouwbare analyse hierop te doen.
Hoewel dit tijdens het onderzoek is gebleken zijn de artefacten
hieronder aangeboden voor analyses op een later moment. Zo kan er
bijvoorbeeld achterhaald worden of er bepaalde momenten in het jaar zijn
waar er meer wordt gezocht naar data professionals.

In de eerste grafiek hieronder is een negatieve trend te zien wanneer er
gekeken wordt naar de totale instroom aan "data" vacatures. De tweede
grafiek toont een autocorrelatie. Op de x-as zijn de dagen vanaf het
huidige datapunt gemeten (0 is dezelfde dag). Op de y-as is de mate van
correlatie. Alle punten boven of onder de blauwe lijnen zouden statisch
significant moeten zijn. Op het moment van schrijven lijkt er een
significant patroon op te treden iedere 6 dagen. Echter is de correlatie
slechts 0.3 wat aanduidt dat het patroon erg zwak is.

#### Instroom vanaf 5 februari 2022

```{r date counts}

# filter data to contain only points after 5th of february
plot_indeed_timeseries_data(df_indeed_final_date_filtered)

```

#### Auto-correlatie instroom

```{r}
#create listing date table
listing_date_table <- table(df_indeed_final_date_filtered$listing_date)

#calculate autocorrelation
acf(listing_date_table, main= "Instroom autocorrelatie")
```

Zoals gesteld kan er met de huidige data weinig betekenis worden gehaald
uit de trendanalyse. Wel biedt de code mogelijkheid voor latere
toepassing. Zo kan er na een langere termijn van scraping een analyse
worden gedaan per opdracht gever of functietype. Ook kan er gekeken
worden naar de algehele markt ontwikkeling en de beste momenten om
opdrachtgevers te benaderen. Dit zouden dan ook aanbevelingen zijn voor
een vervolgonderzoek of toepassing van de bovenstaande grafieken en
bijbehorende code.
