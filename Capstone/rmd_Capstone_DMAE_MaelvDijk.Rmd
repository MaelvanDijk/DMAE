---
title: "Capstone_MaelvDijk_DMaE"
author: "Maël"
date: '2022-03-15'
output: 
  html_document:
    toc: true
    toc_float: true
    highlight: tango
    theme: journal
    code_folding: hide
    # number_sections: true
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE
  )

library("tidyverse")
library("stringr")
library("tseries")


#custom functions
source('./supporting_code/Scraped_indeed_data_wrangler.R')
source('./supporting_code/Scraped_indeed_plotter.R')
source('./supporting_code/Kaggle_indeed_plotter.R')
```

## Samenvating

*Samenvatting (max. 250 woorden)*

## Inleiding

---
# Inleiding
# 
#     Geef de context van het project (aanleiding, kwestie etc.)
#     Sluit aan bij Business Understanding van CRISP-DM
#     Formuleer een hoofdvraag en zo nodig deelvragen
#     Noem de belangrijkste databronnen
---

In dit capstone project wordt de ontwikkeling naar de vraag van
hardskills en rollen binnen het data domein onderzocht. Het project is
ontsprongen naar aanleiding van een gerelateerde vraag vanuit House of
Bèta, namelijk: een onderzoek naar de drivers en kenmerken van onze
consultants die uitstromen. Wegens juridische beperkingen en het hier
uit volgende tijdsgebrek is gekozen voor het onderzoeken van een
alternatieve vraag, namelijk: **"Hoe ontwikkelt de vraag naar 'data
specialisten' zich kijkende naar de gevraagde vaardigheden en het
geboden loon"**.

Deze nieuw geformuleerde vraag helpt deels de originele vraag te
beantwoorden. De resultaten kunnen namelijk dienen als benchmarks voor
het geboden loon en ondersteunen in het samenstellen van een (deel van)
het arbeidsvoorwaarden pakket. Een derde belang is het peilen van de
vraag naar "data personeel" en de daarbijhorende opdrachtgevers, hiermee
kan dan ingespeeld worden op de veranderende markt door de
opleidingsstraat en specialisaties beter te laten aansluiten op de markt
van morgen.

### House of Bèta

House of Bèta (HoB) is een detacheerder en consultancy op het gebied van
business IT. Er zijn diverse specialisatie richtingen waartussen een
consultant kan kiezen. HoB richt zich op starters op de arbeidsmarkt en
staat open voor consultants met en zonder formele IT opleiding. Naast
het matchen tussen vraag en aanbod van consultants en opdrachtgevers
zorgt HoB voor een breed aanbod aan opleidingen zodat consultants zich
kunnen blijven ontwikkelen op het gebied van hard-, en softskills. Een
consultancy staat of valt met de expertise en kennis van haar
medewerkers, dit maakt retentie van medewerkers belangrijk. De uitstroom
van HoB's zusterbedrijf Talent&Pro in 2021 is aanleiding voor HoB om
hier meer grip op te krijgen. Na een kort voor onderzoek is gebleken dat
met de beschikbare tijd en midellen de uitstroom analyseren en
potentiëel voorspellen nog niet mogelijk is. Hierop is voorgesteld om
naar de vacature ontwikkeling te kijken. Deze analyse kan een bijdragen
leveren aan het onderzoek omtrent uitstroom, daarnaast geeft het HoB de
mogelijkheid om haar huidige compensatie aan medewerkers op waarde te
schatten.

### Businessvraag

De vraag die bij de business ligt: **"Hoe positioneert HoB wat betreft
het loon ten opzichten van de ontwikkeling van de vraag naar 'data
specialisten'in Nederland zich kijkende naar de gevraagde vaardigheden,
het geboden loon en mogelijke opdrachtgevers?"** komt vanuit de
commercieel directeur van HoB. Het probleemgebied heeft betrekking op:
commercie, businessdevelopement en HR. Dit maakt dat de uiteindelijke
inzichten op meer plekken in de organisatie gedragen zal worden.

Om deze vraag te beantwoorden moeten er een aantal deelvragen beantwoord
worden:

1.  Hoe verhoudt het loon van HoB zich tot de Nederlandse banenmarkt op
    het gebied van data specialisten?

2.  Hoe ontwikkelt de vraag naar individuele skills zich op de
    nederlandse arbeids markt?

3.  Waar zitten de grootste ontwikkeling op gebied van rollen, skills en
    opdrachtgevers

### data gebruik

Er vinden nog geen datamining activiteiten plaats binnen HoB. Naast de
analyse wordt er dan ook een stuk code geleverd om een vacatures te
minen. Dit kan vrij door HoB gebruikt worden indien gewenst. Omdat voor
dit capstone project de tijd beperkt is zijn er een aantal keuzes
gemaakt m.b.t de data en het minen hier van: 1. Voor vacatures wordt
alleen data van Indeed gebruikt 2. De vacatures zijn beschikbaar vanaf 8
februari 2022 3. Als referentie dataset voor benodigde (hard)skills
wordt de [Indeed dataset van
Kaggle](https://www.kaggle.com/elroyggj/indeed-dataset-data-scientistanalystengineer)
gebruikt.

### Business Success criteria

Kwantitatieve doelstelling voor het beantwoorden van deze vraag is niet
direct mogelijk. Daarom wordt er gekeken naar kwalitatieve eisen.
Minimaal wordt er een inkomens benchmark verwacht en een indicatie van
de hoeveelheid vraag bij bedrijven. Een bijkomende wens is om het
arbeidsvoorwaardenpakket op waarden te kunnen schatten. Dit is een
uitdaging waar nog niet duidelijk van is of dit gaat lukken.

## Methoden

Het hoofdstuk "Methoden" gaat in op de data en het gebruik hiervan. Het
is van belang om te weten hoe er aan de data is gekomen, met wat voor
data er gewerkt wordt en hoe er uiteindelijk is gekomen tot een
bruikbaar datamodel. Dit wordt achtereenvolgens uiteengezet in de
paragagraven: "Data Understanding", "Data Preperation en Modelling". Als
laatste wordt er in paragraaf "Reproduceren van het onderzoek" ingegaan
op de eisen en benodigdheden om tot eenzelfde resultaat te komen.

---
# Methoden
# 
#     Sluit aan bij Data Understanding, Data Preparation en Modelling van CRISP-DM
#     Presenteer hier eventuele tussenresultaten (bv. over datakwaliteit/-profilering)
#     Beschrijf wat essentieel is voor het reproduceren van het werk
---

### Data understanding

Deze paragraaf gaat in op de scope en basale kenmerken van de gebruikte
data. Er wordt beschreven met welke datasets gewerkt gaat worden, de
redernatie voor deze datasets en waarom andere datasets buiten
beschouwing zijn gelaten. Uiteindelijk wordt de data in zijn ruwe vorm
beschreven.

#### Gekozen datasets en scope

Om te beginnen is hieronder een versimpelde datalog weergegeven. Dit
bevat informatie over de datasets in de ruwevorm (te vinden in
"*.\\Data_raw")"*

| ID   | dataset name                   | format | beschrijving                                                                         | dataset owner |
|------------|------------|------------|-------------------------|------------|
| D001 | indeed_job_dataset.csv         | CSV    | dataset met job post data vanuit de VS voor data anlysten, scientisten en engineers. | Elroy         |
| D002 | <DATE> scraped indeed data.csv | CSV    | scraped data voor functies met het woord "data" in nederland                         | Maël          |

: datalog (shortened)

D001 is na vooronderzoek gekozen als basis dataset. Hierin staan
onderandere (hard)skills met de daarbij behorende functie type (data
analist, -engineer, -scientist). Dit legt een goede basis voor het
onderzoek naar data specialisaties binnen HoB, hier worden namelijk
dezelfde specialisaties ook aangeboden. Er zijn andere datasets
overwogen echter bevatte deze te weinig relevantie functie type (\< 200
rijen) of ontbraken hier gevraagde skills die ter referentie voor D002
gebruikt zouden worden.

D002 is gescrapte data van Indeed. Het rationaal achter het zelfstandig
scrapen van Indeed data komt voort uit meerdere beperkingen. Allereerst
was er geen (makkelijk en gratis) werkende API te vinden die Indeed data
kon aanleveren. Ten tweede was er geen dataset beschikbaar die focust op
data specialisten én de nederlandse markt. Als laatste waren de
beschikbare datasets vaak te beperkt met relevante informatie
(ontbrekende skills, niet nederlandse functies, geen salaris indicatie).
Dit allemaal heeft er toe geleid dat er voor dit project een scraper is
gebouwd die de data binnenhaalt.

#### Beschrijving van de ruwe data

De data van zowel D001 als D002 zijn nog in ruwe vorm. Bij het
exploreren van D001 is duidelijk dat de data al erg schoon is. Een paar
basale kenmerken zijn hieronder geprint. Deze kunnen zowel snel inzicht
geven in de dataset als relevant zijn voor het gehele onderzoek.

```{r basic D001 data, echo=FALSE}
# kenmerken D001 dataset
df_indeed_kaggle <- read.csv(".\\Data_raw\\D001\\indeed_job_dataset.csv")

# head(df_indeed_kaggle)
# df_indeed_kaggle %>% summary()
writeLines(
  sprintf(
    "Number of columns in the dataset:\n%s\n",
    ncol(df_indeed_kaggle)
  )
)

writeLines(
  sprintf(
    "Number of rows in the dataset:\n%s\n",
    nrow(df_indeed_kaggle)
  )
)

writeLines(
  sprintf(
    "Number of Skill (combinations) in the dataset:\n%s\n",
    df_indeed_kaggle$Skill %>% unique() %>% length()
  )
)

writeLines(
  sprintf(
    "Number of salaries  in the dataset:\n%s\n",
    df_indeed_kaggle$Queried_Salary %>% unique() %>% length()
  )
)

writeLines(
  sprintf(
    "number of observations per job type (analist, engineer, scientist):\n%s",
    aggregate(
      df_indeed_kaggle$Job_Type,
      by=list(df_indeed_kaggle$Job_Type),
      FUN=length
      )[2]
  )
)


```

Er zijn behoorlijk wat kolommen aanwezig in de dataset, echter blijkt
een groot deel niet relevant voor de vraag in kwestie. De dataset bevat
klaarblijkelijk veel skills. Dit komt voornamelijk omdat de skills
opgeslagen zijn in lijsten en een willekeurige volgorde binnen deze
lijsten hebben. Dit zorgt ervoor dat bijna elke vacature een unieke
skill of combinatie van skills lijkt te bevatten. In werkelijkheid is
dit niet zo want [SQL, Python] is gelijk aan [Python, SQL]. HIervoor
komt later in het project een ETL process. Het salaris is ook opvallend.
De maker van deze dataset heeft de salarissen in bins van \$20.000,-
geplaatst (met een groep \< \$80.000 en een groep \> \$160.000,-).
Vercer valt nog op dat Data Engineer slechts 24% van de dataset opmaakt.
Daarentegen zijn 45% van de listings voor Data Scientisten.

Dataset D002 bevat data gescraped van Indeed. Hiervoor is er gezocht
naar alle functies waar het woord "data" in voorkomt. D002 bestaat in de
ruwe vorm uit meerdere losse bestanden (een voor elke dag dat er een run
is geweest van de scraper). Om een indruk te krijgen van wat er in een
dergelijk bestand staat kan er gekeken worden naar
*".\\Data_raw\\D002\\2022-03-08 scraped indeed data.csv"*. Hieronder
worden weer de voornaamste kenmerken weergegeven

```{r basic D002 data, echo=FALSE}
# kenmerken D002 dataset
df_indeed_scraped <- read.csv(".\\Data_raw\\D002\\2022-03-08 scraped indeed data.csv")

writeLines(
  sprintf(
    "Number of columns in the dataset:\n%s\n",
    ncol(df_indeed_scraped)
  )
)

writeLines(
  sprintf(
    "Number of rows in the dataset:\n%s\n",
    nrow(df_indeed_scraped)
  )
)

writeLines(
  sprintf(
    "number of unique skill(s) and combinations:\n%s\n",
    df_indeed_scraped$skills %>% unique() %>% length()
  )
)

writeLines(
  sprintf(
    "number of unique URL's:\n%s\n",
    df_indeed_scraped$job_link %>% unique() %>% length()
  )
)

writeLines(
  sprintf(
    "Example of salary:\n%s\n",
    df_indeed_scraped$salary[2]
  )
)

writeLines(
  sprintf(
    "number of NA's:\n%s\n",
    sum(is.na(df_indeed_scraped))
  )
)

writeLines(
  sprintf(
    "earliest job listing:\n%s\n",
    df_indeed_scraped$listing_date %>% min(na.rm = TRUE)
  )
)

writeLines(
  sprintf(
    "Skills example:\n%s\n",
    df_indeed_scraped$skills[2]
  )
)
```

Uit bovenstaande informatie vallen een aantal dingen op te maken.
Allereerst gaat de data niet verder terug dan begin februari 2022. Dit
komt omdat de webscraper pas op 6 maart 2022 is afgerond. Dit zal ook
effect hebben op de analyses gezien de data niet ver genoeg teruggaat om
echte trends te ontdekken. Ten tweede valt op dat het aantal rijen en
het aantal unieke URL's niet overeenkomen. Dit geeft het vermoeden dat
er dubbele rijen zijn, of in iedergeval een functie meermaals is
gescraped. Ook valt op dat er een fout in de salaris kolom staat. Hierin
is het euro-symbool (€) niet goed vertaald is (ï¿½). Dit en nadere
opschoning vindt plaats in de volgende paragraaf "Data preperation en
modeling".

### Data preperation en modeling

Voor het binnenhalen, schoonmaken en verijken van de data zijn diverse
stappen genomen. In de onderstaande processflow is te zien hoe het
process hoogover doorlopen wordt om tot de uiteindelijke analyses te
komen.

Het process is in 3 segmenten op te delen. Het bovenste segment heeft
betrekking op de data vanuit Kaggle (D001), het tweede segement, in het
midden, heeft betrekking op de Indeed data die gescraped wordt (D002)
vervolgens is de onderste laag waar alles samenkomt. Na het schonen
verijken en wegschrijven van de data kan er een dataframe worden opgezet
die goed interpeteerbaar is.

![Data process flow](Images/data%20flow%20process.png)

Er zijn een aantal belangrijke kanttekenen die moeten worden gemaakt.
Wegens het gebrek aan tijd zijn een aantal aannamens gedaan en is data
versimpeld weergegeven. De meest noemenswaardigen zijn:

| Process stap        | Betreffende kolom | Versimpelde weergaven                                                                                                                                                                                                                                                                               |
|-------------|-------------|----------------------------------------------|
| Scraping            | Salary            | Er is bij weergaven salarissen altijd voor het laagste salaris gegaan                                                                                                                                                                                                                               |
| ETL (midden-rechts) | job_type          | Er zijn maar 3 types gedefinieëerd (data analist, scientist, en engineer). Ongeacht de functietitel zal 1 van deze types worden toegewezen. Zo kan data architect of teamleider data analytics worden toegewezen aan het type data analist.                                                         |
| ETL (midden-rechts) | salary            | er is aangenomen dat alles boven de €10.000,- jaarsalaris betreft. dit wordt omgerekend naar maand. Alles onder de €100,- is als uur tarief aangenomen en wordt omgerekend naar maan (obv. 168 uur per maand). alles tussen de €100,- en €1000,- wordt als scraping fout gezien en omgezet naar NA. |

: Aannamens tijdens process stappen,

De scripts voor deze stappen zijn te vinden in "./Supporting_code",
specifiek:

| Process stap        | Script naam                    |
|---------------------|--------------------------------|
| ETL (linksboven)    | Generate_ref_skill_list.R      |
| Scraping            | indeed_scraper.R               |
| ETL (midden-rechts) | Scraped_indeed_data_wrangler.R |

: process stappen en bijbehorende scripts.

Het draaien van deze scripts zorgt uitindelijk voor een dataframe dat
gebruikt kan worden voor het analyseren en beantwoorden van de
businessvraag. Voor een duidelijker beeld wat de code doet zijn de .R
scripts voorzien van comments.

### Reproduceren van het onderzoek

Om dit project te reproduceren is het nodig om de mappen:
"Data_cleaned", "Data_raw", en "Supporting_code" te gebruiken. Hierin
staan bron data en hulp .R scripts. In deze Markdown worden hulp scripts
aangeroepen om de code complexiteit in deze Markdown te beperken. Wat
betreft externe libraries moeten de volgende worden geïnstalleerd:

"tidyverse", "stringr", "lubridate", "tibble", "textTinyR". Indien er
nieuwe data gescraped moet worden (niet nodig voor reproductie): "XML"
en "rvest".

Er zijn verder geen afhankelijkheden buiten het bovengenoemde om deze
markdown te draaien.

## Resultaten

---
# Resultaten
# 
#     Sluit aan bij Evaluation van CRISP-DM
#     Presenteer de resultaten die nodig zijn voor het beantwoorden van de onderzoeksvragen
#     Ondersteun de tekst met figuren en tabellen
---

In dit hoofdstukworden de deelvragen beantwoord aan de hand van de
analyse resultaten. Ook wordt uiteengezet welke vragen niet beantwoord
kunnen worden en welke artifacten opgeleverd worden. Als eerste wordt
gekeken naar de verhouding van het HoB salaris t.o.v. het salaris op
Indeed, vervolgens wordt gekeken

```{r clean and enrich D002, echo=FALSE}
# clean en verrijk indeed data met custom functies
df_indeed_merged <- merge_scraped_data()
df_indeed_cleaned <- clean_scraped_date(df_indeed_merged)
df_indeed_final <- enrich_scraped_date(df_indeed_cleaned)

df_indeed_skills <- unnest_skills(df_indeed_final)

df_indeed_final_date_filtered <- filter_dates(df_indeed_final)
df_indeed_skills_date_filtered<- filter_dates(df_indeed_skills)

rm(
  df_indeed_kaggle,
  df_indeed_scraped,
  df_indeed_merged,
  df_indeed_cleaned
)
```

### Hoe verhoudt het loon van HoB zich tot de Nederlandse banenmarkt op het gebied van data specialisten? {.tabset}

Binnen HoB is het mogelijk om op verschillende manieren salarisverhoging
te behalen. Zo wordt een consultant beloont voor het behalen van
cursussen en opleidingen en hangt de verhoging samen met de beoordeling.
Afhankelijk van het niveau van de consultant is dit een absolute
stijging of een relatieve stijging ten opzichten van het huidige loon.
Ook worden consultants beloond voor het behalen van cursussen. Om deze
reden is het lasting om een accuraat salaris te bepalen. Gelukkig heeft
HoB op haar website de referentie salarissen voor 1ste, 2de en 3de jaars
consultants staan (€2.763,- , €3.087,- , €3.591,- respectievelijk).

De vraag van HoB is om dit geboden salaris aanbod op waarden te schatten
ten opzichten van de markt. In onderstaande grafiek is te zien dat het
HoB salaris de eerste twee jaren lager ligt dan het mediaan en modale
salaris aangeboden op Indeed.

```{r plot indeed salary distribution vs hob salary, echo=FALSE}
plot_hob_salary_vs_indeed_salary_dist(df_indeed_final)
```

In de bovenstaande analyse is het het aantal jaren werkervaring van de
indeed vacatures niet meegeneomen. De dataset is wel vereikt met een
kolom die het aantal jaren ervaring aangeeft. Echter, doordat er 'regex'
is gebruikt en de data erg ongestructureerd is zal deze kolom niet
helemaal betrouwbaar zijn. Desondanks kan deze data gebruikt worden om
toch een indruk te krijgen van de salarisverdeling over de verschillende
eisen qua jaren werkervaring. In onderstaande grafiek komt duidelijk
naar voren dat het salaris van HoB personeel in jaar 3 hoger ligt dan
het modale salaris dat op indeed geboden wordt tot en met jaar 3. Ook is
opvallend dat bij 8 jaar werkervaring de curve naar links verschuifd

```{r HoB salaris vs Indeed verdeling per ervaringsjaar, echo=FALSE}
df_indeed_final %>%
    ggplot(aes(x= salary)) +
    geom_density(alpha=0.5) +
    geom_segment(
      aes(
        x =2763, xend =2763,
        y= 0.0000, yend=0.003,
        linetype= "HoB 1ste jaar"
      )
    ) +
      geom_segment(
      aes(
        x =3087, xend =3087,
        y= 0.0000, yend=0.003,
        linetype= "HoB 2de jaar"
      )
    ) +
      geom_segment(
      aes(
        x =3519, xend =3519,
        y= 0.0000, yend=0.003,
        linetype= "HoB 3de jaar"
      )
    ) +
    facet_wrap(vars(cleaned_werkervaring_jaren)) +
    labs(
      title="Verdeling geboden salaris per jaren werkervaring",
      y= "Frequentie (%)",
      x= "Geboden salaris (\u20ac)"
    ) +
    theme(
      plot.title=element_text(hjust=0.5))+
    scale_y_continuous(
      breaks= seq(0.0000, 0.004, 0.001),
      labels=seq(0.00, 0.04, 0.01),
      limits= c(0, 0.003)
    ) +
  xlim(2000,6000)
```

Het salaris bij HoB wordt niet alleen bepaald door werkervaring. Het is
namelijk ook zo dat er veel in de consultants wordt geïnvesteerd
doormiddel van trainingen. HoB richt zich zowel op hard-, als
softskills. Binnen dit onderzoek zijn de hardskills het meest accurate
uit de tekst te halen en daardoor het beste te onderzoeken. Bij HoB zal
een consultant data gemiddeld 3 tot 5 skills overkoepelende skills
machtig zijn, In kader van dit project is niet direct meetbaar welke
skills het meest voorkomen binnen HoB data consultants, echter op
ervaring afgaande zal dit de volgende skills betreffen: SQL, Python, R,
Excel en Data Modeleren. In onderstaande analyse is de salaris verdeling
voor 3, 4, en 5 skills te zien.

Hieronder zijn drie grafieken weergegeven. Iedere grafiek laat het
mediaan salaris per aantal skills zien. De balken zijn groen gekleurd
zodra het referentie salaris van HoB hoger ligt dan het mediaan salaris.
Er vallen hier twee belangrijke dingen op, namelijk:

1.  Er is een klein positief lineair verband tussen het aantal skills en
    het mediaan gebode salaris
2.  Het referentie salaris van HoB is pas vanaf het 3e jaar
    'competitief' te noemen.

Een verdieping zou nog bereikt kunnen worden door ook te kijken naar het
aantal jaren werkervaring als extra dimensie. Helaas zijn er op dit
moment te weinig observaties om deze extra dimensie toe te voegen in
deze analyse.

#### 1ste jaars HoB

```{r}
df_indeed_final %>%
  group_by(skill_count) %>%
  summarise(med= median(salary, na.rm = T)) %>%
  ggplot(
    aes(x=skill_count, y=med)) +
  geom_col(
    aes(
      fill = ifelse(med < 2763, 'red', 'blue')
      )
    )+
  geom_hline(linetype= 1,
             yintercept = 2763
             )+
  labs(
    title = "Mediaan salaris Indeed vs. aantal skills",
    subtitle = "HoB 1ste jaars salaris als referentie lijn",
    y = "Mediaan salaris Indeed",
    x = "Aantal skills"
  ) +
  theme(
    legend.position= "none"
  )
```

#### 2de jaars HoB

```{r}
df_indeed_final %>%
  group_by(skill_count) %>%
  summarise(med= median(salary, na.rm = T)) %>%
  ggplot(
    aes(x=skill_count, y=med)) +
  geom_col(
    aes(
      fill = ifelse(med < 3087, 'red', 'blue')
      )
    )+
  geom_hline(linetype= 1,
             yintercept = 3087
             )+
  labs(
    title = "Mediaan salaris Indeed vs. aantal skills",
    subtitle = "HoB 2de jaars salaris als referentie lijn",
    y = "Mediaan salaris Indeed",
    x = "Aantal skills"
  ) +
  theme(
    legend.position= "none"
  )
```

#### 3de jaars HoB

```{r}
df_indeed_final %>%
  group_by(skill_count) %>%
  summarise(med= median(salary, na.rm = T)) %>%
  ggplot(
    aes(x=skill_count, y=med)) +
  geom_col(
    aes(
      fill = ifelse(med < 3519, 'red', 'blue')
      )
    )+
  geom_hline(linetype= 1,
             yintercept = 3519
             )+
  labs(
    title = "Mediaan salaris Indeed vs. aantal skills",
    subtitle = "HoB 3de jaars salaris als referentie lijn",
    y = "Mediaan salaris Indeed",
    x = "Aantal skills"
  ) +
  theme(
    legend.position= "none"
  )

```

### Vraag naar Skills {.tabset}

De volgende deelvraag voor dit onderzoek gaat om de vraag ontwikkeling
naar specifieke skills. Hoe ontwikkelt de vraag naar specifieke skills
zich over de tijd? Deze deelvraag is lastig te beantwoorden op binnen
dit onderzoek. De data loopt niet verder terug dan begin februari 2022.
Voor een duidelijke trend analyse is er data nodig van \~3 jaar. Dit
maakt dat onderstaande analyses een versimpelde weergaven zullen zijn.
De artefacten die aangeleverd worden (waaronder de scraper) kunnen
gebruikt worden om deze dataset verder op te bouwen totdat er genoeg
data is om de onderstaande analyse opnieuw te doen.

Als eerst kan er gekeken worden naar de totale vraag naar skills. In de
twee onderstaande grafieken zijn zowel de 20 meest gevraagde skills als
de 20 minst gevraagde skills terug te vinden. de top drie is hier weinig
verrassend en behelst de toch wel meest bekende talen binnen het data
vakgebied. Verassender is om Tableau in de top 20 te zien maar geen
PowerBI. Verder wordt nu duidelijk wanneer gekeken wordt naar zowel de
top als de bottom 20 dat er een mix van echte hard skills en bepaalde
conventies (zoals "https" en "rest") worden opgenomen in de skill
referentie lijst uit D001. Hier is met namen opvallend dat "marketing"
en "network" hoog scoren. Hierin kan er aangenomen worden dat "network"
het cloud aspect zijn van het data vak, maar, het zou ook kunnen duiden
op neurale netwerken. Helaas is dit een fout in het scrapen en binnen
dit project niet direct meer te herstellen. Wat betreft "marketing" kan
verwacht worden dat dit gaat om marketing analytics en marketing
automation. Twee velden die erg groot zijn binnen data science en data
analytics.

Het opmerkelijkste is dat PowerBI niet hoger in de lijst staat. Een van
de platformen waarop HoB haar energie richt wat betreft trainingen. In
tegenstelling is het Tableau waar dagelijks meer vacatures voor
verschijnen. Dit om te overeen met de initiële analyse van D001 waarin
de aannamen was dat de Amerikaanse markt andere voorkeuren heeft dan de
Nederlandse markt, wat aanleiding zou kunnen zijn voor verder onderzoek.

De bottom 20 geeft weinig informatie. Het zijn in meer en mindere maten
bekende talen/tools. het signaleert een niche binnen de vacatures maar
buiten het feit dat HoB haar activiteiten hier niet op moet focussen
levert dit ons geen extra informatie op.

#### top skills

```{r}

all_skills <- df_indeed_skills$skills %>% sort(decreasing= TRUE)

# create a top and bottom 20
top_skills <- table(df_indeed_skills$skills) %>% sort() %>% tail(20)
bottom_skills <- table(df_indeed_skills$skills) %>% sort() %>% head(20)

# plot top skills
df_indeed_skills %>%
    # filter df based on most sought skill
  filter(skills %in% names(top_skills)) %>% 
  ggplot(
      aes(
        x = reorder(
          skills,
          skills,
          function(x) + length(x) # decreasing order skill
          ),
          fill= job_type
          )
      ) +
  geom_bar() +
  theme(
    axis.text.y = element_text(
      face= 'bold',
      size= 14)) + # Rotate x-labels and change font
      coord_flip() +
  labs(
    x= "Skill"
  )


```

#### Bottom skills

```{r}

# plot bottom skills
df_indeed_skills %>%
    # filter df based on least sought skill
  filter(skills %in% names(bottom_skills)) %>% 
  ggplot(
      aes(
        x = reorder(
          skills,
          skills,
          function(x) + length(x) # decreasing order skill
          ),
          fill= job_type
          )
      ) +
  geom_bar() +
  theme(
    axis.text.y = element_text(
      face= 'bold',
      size= 14)) + # Rotate x-labels and change font
      coord_flip() +
  labs(
    x= "Skill"
  )
```

### Instroom vacatures {.tabset}

Een volgende stap is patroon analyse. Omdat de data niet vergenoeg terug
gaat kan er geen accurate voorspelling worden gedaan over de
ontwikkeling naar de vraag. Naast dat het inspelen op deze vraag
ontwikkeling ook erg lastig is wegens de doorlooptijd van het opzetten
van opleidingen en het opleiden van kandidaten. Wel kan er gestart
worden met een analyse of er een bepaald patroon is m.b.t. de instroom.

De eerste grafiek hieronder maakt duidelijk dat er een grote instroom
van vacatures is op het eerste moment van scrapen. Dit heeft een
logische verklaring: Indeed kent drie soorten waarden m.b.t plaatsings
moment "Vandaag", "1-30 dagen geleden" "30+ dagen geleden". De scraper
maakt geen onderscheid tussen "30" en "30+" Dit betekent dat het eerste
moment van scrapen alle "30" en "30+" kenmerken om worden gezet naar
"30". Dit verklaart een spike wanneer deze twee waarden voorkomen,
doordat tijdens het data schonen dubbele rijen worden verwijderd (de
eerste wordt behouden) zien we dit effect alleen terug in het begin van
de totale dataset. Wanneer er dus gekeken wordt naar de vraag over tijd
zal het binnen deze dataset noodzakelijk zijn om vanaf 5 februari 2022
te kijken. In beide grafieken is lastig vast te stellen of er een
correlatie of een bepaald patroon aanwezig is. Wel valt op te maken dat
er over het algemeen een dalende trend lijkt te zijn. Om hier echter
meer over te zeggen zal minstens 3 jaar aan data nodig zijn om trends en
patronen te kunnen vaststellen.

Beide grafieken lijken pieken op bepaalde intervallen te tonen. In de
derde grafiek wordt daarom de autocorrelatie weerggeven hieruit blijkt
dat er geen duidelijk patroon aanwezig is, hierbij zou 1 (of -1) een
mogelijke correlatie of patroon aanduiden. Deze vindt alleen plaats op
lag 0. Dit betekent dat er alleen een verband is op de dag zelf. Kortom
lijkt er geen patroon zichtbaar te zijn wat betreft de instroom.

#### Instroom vanaf begin

```{r unfilterd date counts}

plot_indeed_timeseries_data(df_indeed_final)

```

#### Instroom vanaf 5 februari 2022

```{r filtered date counts}

# filter data to contain only points after 5th of february


plot_indeed_timeseries_data(df_indeed_final_date_filtered)

```

#### Auto-correlatie instroom

```{r}
#create listing date table
listing_date_table <- table(df_indeed_final_date_filtered$listing_date)

#calculate autocorrelation
acf(listing_date_table, main= "Instroom autocorrelatie")
```

### Ontwikkeling skill behoeften {.tabset}

Hoewel er met de huidige data nog geen overkoepelend patroon te
signaleren is kan er wel gekeken worden naar de ontwikkeling van de
skill behoeften. Ook hier mist er een hoeveelheid data waarmee een goede
tijdanalyse gedaan kan worden. Daarom dienen onderstaande grafieken en
bijbehorende code ook als artefact waar op een later moment gebruik van
gemaakt kan worden. In onderstaande grafieken zijn in iedergeval de drie
meest gebruikte programmeertalen rondom data specialisaties te vinden.
Daarnaast is het analyse platform Tableau weergegeven en de twee cloud
oplossingen AWS en Azure.

Ook is hier weer een dalende trend te zien in nagenoeg alle skill
aanvragen. Zowel in onderstaande grafieken als in het overzicht van
totale aanvragen kan dit te verklaren zijn door de rijkwijdte van het
scraping script (max 300 vacatures) en de willekeurige sortering op de
Indeed. Daarom zal ook hier de tijd een belangrijke factor spelen en een
goede analyse na 3 jaar data verzamelen gedaan kunnen worden.

Wat we uit deze en de voorgaande analyse in iedergeval kunnen opmaken is
het feit dat er een daling is in vacatures na een piek aan het begin van
februari.

#### SQL

```{r SQL instroom}
plot_indeed_timeseries_data(df_indeed_skills_date_filtered, "sql")
```

#### Python

```{r Python instroom}
plot_indeed_timeseries_data(df_indeed_skills_date_filtered, "python")
```

#### R

```{r R instroom}
plot_indeed_timeseries_data(df_indeed_skills_date_filtered, "r")

```

#### Tableau

```{r Tableau instroom}
plot_indeed_timeseries_data(df_indeed_skills_date_filtered, "tableau")
```

#### Azure

```{r Azure instroom}
plot_indeed_timeseries_data(df_indeed_skills_date_filtered, "azure")
```

#### AWS

```{r AWS instroom}
plot_indeed_timeseries_data(df_indeed_skills_date_filtered, "aws")
```

### Ontwikkeling opdrachtgevers {.tabset}

De laatste deelvraag voor dit onderzoek betreft de ontwikkeling en
verhouding van de vraag vanuit mogelijke opdrachtgevers. Omdat het
gebleken is dat de ontwikkeling van de vraag te weinig zegt wordt hier
alleen nog gekeken naar de totale vraag vanuit opdrachtgevers. Hierop
kunnen we binnen een aantal elementen segmenteren, namelijk: functietype
en gevraagde skills.

Allereerst wordt gekeken naar de 25 bedrijven met het grootste aanbod .
in de eerste grafiek is te zien dat er overwegend gevraagd wordt naar
data scientisten, vervolgens data analisten, en in veel mindere maten
naar data engineers. Wanneer andere detacheerders / uitzendbureaus /
consultancies worden weggelaten blijft de volgende top 5 bedrijven over
(ten tijden schrijven):

1.  Bertelsmann
2.  NN
3.  Rabobank
4.  Belastingdienst
5.  Rijkswaterstaat

Vanuit bovenstaande groep werkt HoB nog niet samen met Bertelsmann (een
mediaconglomeraat) wat zou kunnen duiden op mogelijkheden tot een nieuwe
samenwerking.

In de volgende twee grafieken zijn de top 10 bedrijven (van de top 25)
gekozen die zich niet richten op detachering, consultancy of
traineeships. Hierin is de verdeling over functie type en de 10 meest
gevraagde skills te zien. Het is duidelijk te zien dat zowel de functie
type en de gevraagde vaardigheden in zekere zinnen overeenkomen met de
eerder vastgestelde verdeling over de gehele gescrapte populatie (een
paar afwijkingen achterwege gelaten). Zo lijkt de data analist het meest
gezocht te worden, gevolgd door data scientist en als laatste de data
engineers. Wat betreft vaardigheden komen de top 3 vaardigheden overeen
met de algehele populatie. Het meest opvallende binnen de vaardigheden
zijn "marketing" en "Tableau". Deze zijn niet veel vertegenwoordigd in
de vacatures geplaatst door de meest frequente opdrachtgevers.

Wanneer we echter naar de vierde grafiek kijken zien we de meest
gevraagde skills door de top 10 opdrachtgevers. Het beeld lijkt hier
vertekend door Bertelsmann omdat deze een groot deel van de aanvragen
uit heeft staan. Des al niet te min blijft de top 4 (sql, python, R,
(neural) network) ongewijzigd. Hier verder op in gaande is ook te zien
data Bertelsmann in zet op big data technologie (Hive, scala en spark).
Dit zijn interessante signalene komende van de grootst potentiele
opdrachtgever op Indeed.

#### top 20 bedrijven - functie types

```{r}

top_companies <- table(df_indeed_final$Company) %>% sort() %>% tail(25)

df_indeed_final %>%
  filter(Company %in% names(top_companies)) %>%
  ggplot(
    aes(
        x = reorder(
          Company,
          Company,
          function(x) + length(x) # decreasing order company
        ),
        fill= job_type
    )
  ) +
  geom_bar() +
  coord_flip() +
  labs(
    titel= "Aantal vacatures per bedrijf per funtietype",
    x= "Bedrijf"
    )
```

#### top 10 bedrijven - functie types

```{r}
# get top 10 companies not being consultancies and others alike
companies_of_interest <- c(
  "Bertelsmann SE & Co. KGaA - Corporate Center",
  "NN Group",
  "Rabobank",
  "belastingdienst",
  "Rijkswaterstaat",
  "Alliander",
  "MN",
  "ING",
  "Amarant",
  "ABN AMRO"
)

df_indeed_final %>%
  filter(Company %in% companies_of_interest) %>%
  ggplot(
    aes(
        x = reorder(
          Company,
          Company,
          function(x) + length(x) # decreasing order company
        ),
        fill= job_type
    )
  ) +
  geom_bar() +
  coord_flip() +
  labs(
    titel= "Aantal vacatures per bedrijf per funtietype",
    x= "Bedrijf"
    )
```

#### top 10 bedrijven - top 10 skills (gehele populatie)

```{r}


important_skills_full_pop <- table(df_indeed_skills$skills) %>% sort() %>% tail(10)

df_indeed_skills_date_filtered %>%
  filter(
    Company %in% companies_of_interest &
    skills %in% names(important_skills_full_pop)) %>%
  ggplot(
    aes(
        x = reorder(
          skills,
          skills,
          function(x) + length(x) # decreasing order company
        ),
        fill= Company
    )
  ) +
  geom_bar() +
  coord_flip() +
  labs(
    titel= "Aantal vacatures per bedrijf per funtietype",
    x= "Bedrijf"
    )
```

#### Top 10 bedrijven - top 10 skills

```{r}

df_important_companies <- df_indeed_skills_date_filtered %>%
  filter(Company %in% companies_of_interest)

important_skills <- table(df_important_companies$skills) %>%
  sort() %>%
  tail(10)

df_indeed_skills_date_filtered %>%
  filter(
    Company %in% companies_of_interest &
    skills %in% names(important_skills)) %>%
  ggplot(
    aes(
        x = reorder(
          skills,
          skills,
          function(x) + length(x) # decreasing order company
        ),
        fill= Company
    )
  ) +
  geom_bar() +
  coord_flip() +
  labs(
    titel= "Aantal vacatures per bedrijf per funtietype",
    x= "Bedrijf"
    )
```

Bovenstaande geeft handvatten om te bepalen welke skills momenteel
belangrijk zijn om te ontwikkelen onder consultants en welke
opdrachtgevers benaderd kunnen worden wegens het aantal vacatures. De
code en artefacten die dit document opleveren geven de mogelijkheid om
naar voorkeur de analyse aan te passen door bijvoorbeeld de focus van de
bedrijven te verleggen (indien grootste aanbieders van functies over de
tijd wijzigt).

### Ontwikkeling functie type en vraag opdrachtgevers {.tabset}

Ondanks dat het niet mogelijk is om nu nog een uitspraak te doen over de
ontwikkeling van vraag bij opdrachtgevers wordt er wel vast twee
artefacten aangeleverd in de vorm van een tijdserie op basis van
functietype en een op basis van opdrachtgevers. Deze grafieken en de
bijbehorende code zijn hieronder en in de ondersteunende scripts terug
te vinden voor toekomstig gebruik. Omdat er nu geen uitspraken worden
gedaan i.v.m de statistische validiteit zal er geen toelichting bij de
grafieken te vinden zijn.

#### tijdreeks - functie type

```{r}
df_indeed_final_date_filtered %>%
  group_by(
    listing_date,
    job_type) %>%
  summarise(
    req_per_day= n(),
    .groups= "keep") %>%
  ggplot(
    aes(
      x= listing_date,
      y= req_per_day
      )) +
  geom_line() +
  facet_wrap(
    vars(job_type),
    ncol=1) +
  stat_smooth(
    method = "lm",
    se= FALSE
    ) +
  scale_x_date(
    date_labels = "%a\n%d-%m",
    date_breaks = "week"
               )
```

#### tijdreeks - opdrachtgevers

```{r}


df_indeed_final_date_filtered %>%
  filter(Company %in% companies_of_interest) %>%
  group_by(Company, weeknum) %>%
  summarise(req_per_week = n(), .groups='keep') %>%
  ggplot(
    aes(
      x= weeknum,
      y= req_per_week,
        )
  ) +
  geom_line()
# facet_wrap(vars(Company), ncol=2)
```

## oude code en tekst

------------------------------------------------------------------------

------------------------------------------------------------------------

------------------------------------------------------------------------

------------------------------------------------------------------------

------------------------------------------------------------------------

```{r Indeed salaris verdeling per skill, echo=FALSE}

df_indeed_rel_skill <- df_indeed_final[df_indeed_final$skill_count %in% c(3,4,5), ]

df_indeed_rel_skill %>%
    ggplot(aes(x= salary)) +
      geom_density(alpha=0.5) +
      geom_segment(
        aes(
          x =2763, xend =2763,
          y= 0.0000, yend=0.0001,
          linetype= "HoB 1ste jaar"
        )
      ) +
        geom_segment(
        aes(
          x =3087, xend =3087,
          y= 0.0000, yend=0.0001,
          linetype= "HoB 2de jaar"
        )
      ) +
        geom_segment(
        aes(
          x =3519, xend =3519,
          y= 0.0000, yend=0.0001,
          linetype= "HoB 3de jaar"
        )
      )+
      facet_wrap(vars(skill_count)) +
      labs(
        title="Verdeling geboden salaris per aantal gevraagde skills",
        y= "Frequentie (%)",
        x= "Geboden salaris (\u20ac)"
      ) +
      theme(
        plot.title=element_text(hjust=0.5))+
      scale_y_continuous(
        breaks= seq(0.0000, 0.0007, 0.0001),
        labels= seq(0.00, 0.07, 0.01)
      )
```

Hieronder wordt de data verkend. D001 is al een beetje geschoond en kan
hieronder makkelijk verkend worden. D002 bevat een ruwe data die niet
goed te gebruiken is. Hiervoor is het script
*'./supporting_code/Scraped_indeed_data_wrangler.R'* beschikbaar. Dit
script bevat functies om de data vanuit de scraper op te schonen zodat
er wel inzichten uit de data gehaald kunnen worden.

#### D001

Voor D001 wordt in dit onderdeel de verdeling bekeken voor het aantal
skills en salaris over de totale dataset en gesegmenteerd op baan type.

**Data Analist**

Wat gelijk opvalt aan deze dataset is dat de data analist de "minst
zware" functie lijkt. Er worden minder skills verwacht van een data
analist en het loon voor deze rol ligt ook doorgaans lager dan de andere
twee rollen.

**Data Engineer**

Het is gelijk duidelijk dat data engineers de minste vraag genieten maar
wel een zwaarder profiel moeten hebben. Het salaris van de data engineer
volgt ongeveer de verdeling van de data scientist. Wel valt op dat
ondanks de lagere hoeveelheid vacatures dat de data engineer het
zwaarste skill profiel dient te hebben.

**Data Scientist**

Naar data scientisten lijkt de hoogste vraag te zijn. Het skill profiel
van de data scientist is gemiddeld de verdeling neigt meer naar links
dan de data engineer. Ondanks dit lijkt de salaris verdeling gelijk te
zijn aan die verdeling van de data engineer.

**Andere inzichten**

Verder is in deze dataset te zien dat, wanneer er om meer skills
gevraagd wordt er een iets hoger salaris wordt geboden. Voor de hoogste
salaris groep lijkt dit echter niet te gelden. De aannamen (die nog
getoetst kan worden) is dat dit gaat om teamleiders of andere management
functies waar hard skills minder van belang zijn.

```{r D001 exploration, echo=FALSE}

kaggle_plots <- plot_kaggle_distribution(df_indeed_kaggle)


```

## Discussie

---
# Discussie
# 
#     Geef een kritische evaluatie van de resultaten (bv. beperkingen of verbeterpunten)
#     Sluit aan bij Deployment van CRISP-DM (tenzij dit echt wezenlijk deel van het project was, dan moet dat in 2. Methoden aan bod komen)
---

## Conclusie

---
# Conclusie en aanbevelingen
# 
#     Beantwoord de hoofdvraag
#     Geef eventuele aanbevelingen (SMART en terzake)
---
