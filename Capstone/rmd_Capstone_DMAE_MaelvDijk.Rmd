---
title: "Capstone_MaelvDijk_DMaE"
author: "Maël"
date: '2022-03-15'
output: 
  html_document:
    toc: true
    toc_float: true
    highlight: tango
    theme: journal
    code_folding: hide
    # number_sections: true
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE
  )

library("tidyverse")
library("stringr")
library("tseries")


#custom functions
source('./supporting_code/Scraped_indeed_data_wrangler.R')
source('./supporting_code/Scraped_indeed_plotter.R')
source('./supporting_code/Kaggle_indeed_plotter.R')
```

## Samenvating

HoB blijkt vanaf haar 3e jaar een competitief salaris te bieden waarin
het meer dan 50% van de markt verslaat. In de eerste 2 jaren verdienen
werknemers boven het modale loon. Hierin is zeker te stellen dat HoB een
competitief salaris biedt. Het blijkt echter dat House of Bèta nog
ruimte heeft om de hard skills van haar personeel te ontwikkeling. Het
belangrijkste zijn skills m.b.t cloud, big data, en geavanceerde data
science skills. Deze skills worden steeds meer gevraagd onder de huidige
opdrachtgevers en potentiële nieuwe opdrachtgevers. Wat betreft nieuwe
opdrachtgevers blijken er kansen te liggen bij Bertelsmann en Amarant.
Als laatste is het belangrijk om de skills sql, python en R te blijven
aanbieden en aansterken.

## Inleiding

Dit onderzoek richt zich op de vraag hoe competitief House of Bèta is op
de markt voor data professionals. Hierin wordt gekeken naar zowel het
salaris (dus wat House of Bèta biedt aan het personeel) als de
mogelijkheden voor House of Bèta om een groter deel van de markt te
bedienen (gevraagde vaardigheden en potentiële opdrachtgevers).

House of Bèta heeft al langer de wens om inzichten te krijgen in de
reden van uitstroom van haar consultants, gezien dit een belangrijke
asset is van de organisatie. Wegens juridische en technische beperkingen
is het echter niet mogelijk om hierop diepgaand onderzoek op te doen.

Een tweede wens is om de markt beter te kunnen bedienen en te groeien
als de leverancier van IT personeel. Na overleg is gebleken dat beide
wensen gekoppeld kunnen worden in dit onderzoek. Het onderzoek helpt
House of Bèta inzichten te krijgen in een ontwikkelende markt, haar
eigen arbeidsvoorwaarden op waarden te schatten en biedt een basis voor
verdere analyse aangaande de churn van haar personeel.

### House of Bèta

House of Bèta (HoB) is een detacheerder op het gebied van (business) IT.
HoB probeert voornamelijk starters op de arbeidsmarkt aan te trekken en
probeert deze te matchen aan passende opdrachtgevers. De consultants
hebben de mogelijkheid zich na verloop van tijd te specialiseren waarin
HoB de trainingen en opleiding faciliteert of betaald.

HoB is een relatief nieuwe naam op de markt maar voert al sinds 2015
haar werkzaamheden uit onder het moederbedrijf Talent&Pro. De uitstroom
van consultants die T&P heeft gezien over 2021 is de aanleiding geweest
om meer grip op deze uitstroom te willen krijgen.

Tijdens de een recente informatieavond van HoB zijn nieuwe
doelstellingen voor 2022 gepresenteerd waar ook de ontwikkeling en groei
van data professionals naar voren komt. De combinatie van de uitstroom
en de doelstellingen vormen de aanleiding voor dit onderzoek.

### Businessvraag

Dit onderzoek en de wensen van HoB kunnen worden vastgelegd in een
centrale vraag: "Hoe positioneert HoB zich ten opzichten van de
Nederlandse arbeidsmarkt van data professionals kijkend naar loon,
vaardigheden en potentiële opdrachtgevers?"

Deze vraag kan verder worden opgedeeld in 3 deelvragen:

1\. Hoe verhoudt het salaris van HoB zich tot de Nederlandse markt?

2\. In hoeverre sluit het "curriculum" van HoB zich aan bij de
ontwikkelende markt?

3\. Welke opdrachtgevers actief op de markt worden nog niet bediend door
HoB?

### Data mining goals

Om tot de businessdoelen te komen zijn er een aantal data mining doelen
behaalt moeten worden.

beginnend bij een dataset ophalen die een corpus van vaardigheden bevat.
Vervolgens moet er vacatures gescraped worden van Indeed. Door de twee
datasets te combineren kan er getextmined worden op de vacatures, dit is
het derde doel. Vervolgens moet met de data verrijkt worden met
classificaties om meer structuur te bieden. moeten analyses en
conclusies uit de data worden opgehaald op basis van segmentatie op
loon, vaardigheden en werkervaring.

### data gebruik

Binnen HoB vinden nog geen datamining activiteiten op dit gebied plaats.
Buiten de resultaten van dit onderzoek worden daarom modulaire en
uitbreidbare scripts aangeleverd die het mogelijk maken om de datamining
activiteiten voor dit onderzoek voort te zetten.

### Business Success criteria

De volgende kwalitatieve eisen worden gesteld: er moet er een analyse
komen van HoB versus markt inkomen, een analyse over de vraag verdeling
over bedrijven en een over skills. Daarnaast moet er een bruikbare
scraper voor vervolg onderzoek worden opgeleverd.

## Methoden

Dit hoofdstuk behandelt de data en het gebruik hiervan. In "data
understanding" worden basale kenmerken van de dataset besproken, "data
preperation en modelling" gaat in op het schonen een verrijken van de
data. Als laatste gaat "reproduceren van het onderzoek" over de
mogelijkheid om dit onderzoek na te bootsten.

### Data understanding

Hieronder worden de scope, de gebruikte datasets en de basale kenmerken
beschreven.

#### Gekozen datasets en scope

Om te beginnen is hieronder een versimpelde datalog weergegeven. Dit
bevat informatie over de datasets in de ruwe vorm (te vinden in
".\\Data_raw")".

| ID   | dataset name                   | format | beschrijving                                                                         | dataset owner |
|----------|----------|----------|--------------------------------|----------|
| D001 | indeed_job_dataset.csv         | CSV    | dataset met job post data vanuit de VS voor data anlysten, scientisten en engineers. | Elroy         |
| D002 | <DATE> scraped indeed data.csv | CSV    | scraped data voor functies met het woord "data" in nederland                         | Maël          |

: datalog (shortened)

D001 is na vooronderzoek gekozen als basis dataset. Hierin staan onder
andere (hard)skills, functie type (data analist, -engineer, -scientist).
Dit legt een goede basis voor het onderzoek naar data specialisaties
binnen HoB, hier worden namelijk dezelfde specialisaties ook aangeboden.
Er zijn andere datasets overwogen echter bevatte deze te weinig
relevantie functie type (\< 200 rijen) of ontbraken hier relevante
skills.

D002 is gescrapte data van Indeed. Het rationaal achter het zelfstandig
scrapen van Indeed data komt voort uit het tekortschieten van bestaande
API's en afwezigheid van relevante datasets.

#### Beschrijving van de ruwe data

Beginnend bij de beschrijving van D001. Deze dataset is al erg
opgeschoond. Een aantal basale kenmerken zijn hieronder programmatische
weergegeven. Allereerst is duidelijk dat er 43 kolommen aanwezig zijn.
Echter voor het doel van dit onderzoek zijn maar twee kolommen echt
interessant. Dit zijn "Skill" en "Job_Type". Hierin staan
respectievelijk een lijst van (hard)skills en de functie soort per
observatie.

```{r basic D001 data, echo=FALSE}
# kenmerken D001 dataset
df_indeed_kaggle <- read.csv(".\\Data_raw\\D001\\indeed_job_dataset.csv")

# head(df_indeed_kaggle)
# df_indeed_kaggle %>% summary()
writeLines(
  sprintf(
    "Number of columns in the dataset:\n%s\n",
    ncol(df_indeed_kaggle)
  )
)

writeLines(
  sprintf(
    "Number of rows in the dataset:\n%s\n",
    nrow(df_indeed_kaggle)
  )
)

writeLines(
  sprintf(
    "Number of Skill (combinations) in the dataset:\n%s\n",
    df_indeed_kaggle$Skill %>% unique() %>% length()
  )
)

writeLines(
  sprintf(
    "Number of salaries  in the dataset:\n%s\n",
    df_indeed_kaggle$Queried_Salary %>% unique() %>% length()
  )
)

writeLines(
  sprintf(
    "number of observations per job type (analist, engineer, scientist):\n%s",
    aggregate(
      df_indeed_kaggle$Job_Type,
      by=list(df_indeed_kaggle$Job_Type),
      FUN=length
      )[2]
  )
)


```

Zoals hierboven getoond komt duidelijk naar voren dat het aantal unieke
"Skills" bijna gelijk is aan het aantal unieke rijen. Dit betekent
echter niet dat er zoveel verschil zit tussen de vacatures. Het is
namelijk zo dat [SQL, Python] en [Python, SQL] hetzelfde betekenen maar
wordt gezien als twee unieke rijen. Deze informatie wordt gebruikt in
latere ETL stappen. Daarnaast is opvallend dat de verdeling tussen data
analist (31%), data engineer (24%) en data scientist (45%) niet volledig
evenredig is.

Dataset D002 bevat datavan Indeed. Hiervoor is er gezocht naar alle
functies waar het woord "data" in voorkomt. D002 bestaat in de ruwe vorm
uit meerdere losse bestanden (een voor elke dag dat er een run is
geweest van de scraper). Om een indruk te krijgen van wat er in een
dergelijk bestand staat kan er gekeken worden naar
".\\Data_raw\\D002\\2022-03-08 scraped indeed data.csv". Hieronder
worden weer de voornaamste kenmerken.

```{r basic D002 data, echo=FALSE}
# kenmerken D002 dataset
df_indeed_scraped <- read.csv(".\\Data_raw\\D002\\2022-03-08 scraped indeed data.csv")

writeLines(
  sprintf(
    "Number of columns in the dataset:\n%s\n",
    ncol(df_indeed_scraped)
  )
)

writeLines(
  sprintf(
    "Number of rows in the dataset:\n%s\n",
    nrow(df_indeed_scraped)
  )
)

writeLines(
  sprintf(
    "number of unique skill(s) and combinations:\n%s\n",
    df_indeed_scraped$skills %>% unique() %>% length()
  )
)

writeLines(
  sprintf(
    "number of unique URL's:\n%s\n",
    df_indeed_scraped$job_link %>% unique() %>% length()
  )
)

writeLines(
  sprintf(
    "Example of salary:\n%s\n",
    df_indeed_scraped$salary[2]
  )
)

writeLines(
  sprintf(
    "number of NA's:\n%s\n",
    sum(is.na(df_indeed_scraped))
  )
)

writeLines(
  sprintf(
    "earliest job listing:\n%s\n",
    df_indeed_scraped$listing_date %>% min(na.rm = TRUE)
  )
)

writeLines(
  sprintf(
    "Skills example:\n%s\n",
    df_indeed_scraped$skills[2]
  )
)
```

Het is gelijk duidelijk dat de data niet verder terug gaat dan 6
februari 2022. Dit zal effect hebben op de trend analyses in dit
onderzoek. Ook wordt duidelijk dat een dataset meer rijen heeft dan
unieke URL's. Dit betekent dat dezelfde URL waarschijnlijke meerdere
keren is gescraped. Het derde feit dat duidelijk wordt, euro symbolen
(€) worden foutief weergegeven (ï¿½). Dit en andere
datakwaliteitsproblemen worden in het ETL proces opgelost.

### Data preperation en modeling

Onderstaande processflow geeft in grote lijnen weer hoe er vanaf de ruwe
data wordt gekomen tot een eindproduct.

Het volledige proces is in drie segmenten op te delen. Bovenin betreft
D001. Segment twee (midden) betreft D002. Onderin staat segment drie
waarin het eindresultaat weergegeven wordt.

![Data process flow](Images/data%20flow%20process.png)

Omdat de tijd en scope beperkt is voor dit onderzoek zijn er een aantal
aannames gedaan om problemen en complexiteit te voorkomen:

| Process stap        | Betreffende kolom | Versimpelde weergaven                                                                                                                                                                                                                |
|----------|----------|---------------------------------------------------|
| Scraping            | Salary            | Er is bij weergaven salarissen altijd voor het laagste salaris gegaan                                                                                                                                                                |
| ETL (midden-rechts) | job_type          | Er zijn 3 type (analist, engineer, scientist). De dataset wordt verrijkt met een van deze type zelfs al wijkt dit af. Zo kan de functie "data entry" als "data analist" worden geclassificeerd.                                      |
| ETL (midden-rechts) | salary            | Salaris \> 10.000 is jaarsalaris of salaris \< 100 is uur tarief. Dit wordt omgerekend naar maandsalaris (delen door 12 of maal 168 respectievelijk). Alles tussen de 100 en 1000 wordt als scraping fout gezien en omgezet naar NA. |

: Aannamens tijdens process stappen,

De scripts voor deze stappen zijn te vinden in "./Supporting_code":

| Process stap        | Script naam                    |
|---------------------|--------------------------------|
| ETL (linksboven)    | Generate_ref_skill_list.R      |
| Scraping            | indeed_scraper.R               |
| ETL (midden-rechts) | Scraped_indeed_data_wrangler.R |

: process stappen en bijbehorende scripts.

De scripts en betreffende functies zorgen voor een dataset die gebruikt
kan worden voor dit onderzoek. De scripts zijn voorzien van helpende
comments om het gebruik te vergemakkelijken.

### Reproduceren van het onderzoek

De mappen: "Data_cleaned", "Data_raw", en "Supporting_code" bevatten
alle data en code die nodig is om dit onderzoek te reproduceren. In dit
markdown document worden de data en benodigde scripts aangeroepen om tot
het eindresultaat te komen. De benodigde R libraries zijn: "tidyverse",
"stringr", "lubridate", "tibble", "textTinyR", "XML", "rvest",
"tseries".

Verder is aan het eind van dit document een hoofdstuk "Addendum"
toegevoegd. Dit zijn stukken code en analyses geschreven ten behoeven
van het onderzoek. Echter wegens het gebrek aan ruimte en data zijn deze
uit het onderzoek gehaald. Dit hoofdstuk kan als referentie worden
gebruikt om inzicht te krijgen in de tussenstappen van dit onderzoek en
de artefacten kunnen eventueel later worden toegepast indien er meer
data beschikbaar komt.

## Resultaten

---
# Resultaten
# 
#     Sluit aan bij Evaluation van CRISP-DM
#     Presenteer de resultaten die nodig zijn voor het beantwoorden van de onderzoeksvragen
#     Ondersteun de tekst met figuren en tabellen
---

Dit hoofdstuk beantwoord de deelvragen aan de hand van diverse analyses.
Ook wordt besproken welke vragen (nog) niet kunnen worden beantwoord en
welke artefacten worden aangeleverd.

```{r clean and enrich D002, echo=FALSE}
# clean en verrijk indeed data met custom functies
df_indeed_merged <- merge_scraped_data()
df_indeed_cleaned <- clean_scraped_date(df_indeed_merged)

# geschoonde en verrijkte dataset
df_indeed_final <- enrich_scraped_date(df_indeed_cleaned)

# dataset voor skill analyse
df_indeed_skills <- unnest_skills(df_indeed_final)

# dataset voor tijdserie analyse
df_indeed_final_date_filtered <- filter_dates(df_indeed_final)
df_indeed_skills_date_filtered<- filter_dates(df_indeed_skills)

rm(
  df_indeed_kaggle,
  df_indeed_scraped,
  df_indeed_merged,
  df_indeed_cleaned
)
```

### Hoe verhoudt het loon van HoB zich tot de Nederlandse banenmarkt op het gebied van data specialisten? {.tabset}

HoB biedt verschillende manieren om salaris verhoging te halen, dit
tezamen met de gesloten HR systemen maakt het lastig om een referentie
salaris te bepalen. In dit geval heeft HoB op haar website aangegeven
wat een collega kan verdienen in jaar 1, 2 en 3 (€2.763,- , €3.087,- ,
€3.591,- respectievelijk). Om deze deelvraag te beantwoorden worden deze
drie salarissen gebruikt.

De grafiek hieronder geeft de salaris verdeling van data vacatures op
Indeed weer. Het eerste wat duidelijk wordt is dat het loon van HoB bij
aanvang al boven modaal ligt. Daarnaast is het salaris in jaar 3 hoger
dan het mediaan salaris op Indeed. Ofwel, HoB biedt een hoger
maandsalaris in jaar 3 dan 50% van de markt (afgaande op Indeed).

```{r plot indeed salary distribution vs hob salary, echo=FALSE}
plot_hob_salary_vs_indeed_salary_dist(df_indeed_final)
```

Bovenstaande analyse zegt echter niet alles. Hierin wordt het 3
verschillende maandsalarissen van HoB vergelijken met de gehele
onderzoekspopulatie. Op Indeed worden diverse ervaringseisen gesteld.
Dit gebeurt in de vorm van "1 -- 3 jaar ervaring" o.i.d. Tijdens het
schonen is altijd de kleinste hoeveelheid opgeslagen (1 -- 3 is
opgeslagen als 1).

Hieronder is de salaris verdeling te zien gesegmenteerd op gevraagde
ervaring. Door de dataset op te delen in negen segmenten (gemiddeld 166
observaties) is de verdeling gevoeliger en minder accuraat. Toch geeft
het de indruk dat het salaris geboden door HoB doorgaans beter is dan
het salaris van Indeed (jaren 6 en 7 buiten beschouwing gelaten).

```{r HoB salaris vs Indeed verdeling per ervaringsjaar, echo=FALSE}
df_indeed_final %>%
    ggplot(aes(x= salary)) +
    geom_density(alpha=0.5) +
    geom_segment(
      aes(
        x =2763, xend =2763,
        y= 0.0000, yend=0.003,
        linetype= "HoB 1ste jaar"
      )
    ) +
      geom_segment(
      aes(
        x =3087, xend =3087,
        y= 0.0000, yend=0.003,
        linetype= "HoB 2de jaar"
      )
    ) +
      geom_segment(
      aes(
        x =3519, xend =3519,
        y= 0.0000, yend=0.003,
        linetype= "HoB 3de jaar"
      )
    ) +
    facet_wrap(vars(cleaned_werkervaring_jaren)) +
    labs(
      title="Verdeling geboden salaris per jaren werkervaring",
      y= "Frequentie (%)",
      x= "Geboden salaris (\u20ac)"
    ) +
    theme(
      plot.title=element_text(hjust=0.5))+
    scale_y_continuous(
      breaks= seq(0.0000, 0.004, 0.001),
      labels=seq(0.00, 0.04, 0.01),
      limits= c(0, 0.003)
    ) +
  xlim(2000,6000)
```

Het salaris bij HoB wordt niet alleen bepaalt door werkervaring. Ook het
afronden van cursussen en het opdoen van skills wordt beloond. Wat
betreft hard skills kan er vanuit worden gegaan dat een consultant 4 tot
6 skills bezit na de eerste 3 jaar bij HoB. Dit zijn data modeling,
excel, R, Python, SQL en PowerBI. In onderstaande drie grafieken is het
mediaan salaris op Indeed weergegeven verdeeld over het gevraagde aantal
skills. De zwarte lijn geeft het bijbehorende HoB salaris aan.

De drie grafieken met elkaar vergeleken laat zien dat HoB vooral vanaf
het 3e jaar echt een competatief salaris biedt. Hierbij kan verwacht
worden dat een consultant zo'n vier tot zes vaardigheden bezit. Daarmee
verslaat het de markt kijkend naar skill,-, loonratio.

#### 1ste jaars HoB

```{r}
df_indeed_final %>%
  group_by(skill_count) %>%
  summarise(med= median(salary, na.rm = T)) %>%
  ggplot(
    aes(x=skill_count, y=med)) +
  geom_col(
    aes(
      fill = ifelse(med < 2763, 'red', 'blue')
      )
    )+
  geom_hline(linetype= 1,
             yintercept = 2763
             )+
  labs(
    title = "Mediaan salaris Indeed vs. aantal skills",
    subtitle = "HoB 1ste jaars salaris als referentie lijn",
    y = "Mediaan salaris Indeed",
    x = "Aantal skills"
  ) +
  theme(
    legend.position= "none"
  )
```

#### 2de jaars HoB

```{r}
df_indeed_final %>%
  group_by(skill_count) %>%
  summarise(med= median(salary, na.rm = T)) %>%
  ggplot(
    aes(x=skill_count, y=med)) +
  geom_col(
    aes(
      fill = ifelse(med < 3087, 'red', 'blue')
      )
    )+
  geom_hline(linetype= 1,
             yintercept = 3087
             )+
  labs(
    title = "Mediaan salaris Indeed vs. aantal skills",
    subtitle = "HoB 2de jaars salaris als referentie lijn",
    y = "Mediaan salaris Indeed",
    x = "Aantal skills"
  ) +
  theme(
    legend.position= "none"
  )
```

#### 3de jaars HoB

```{r}
df_indeed_final %>%
  group_by(skill_count) %>%
  summarise(med= median(salary, na.rm = T)) %>%
  ggplot(
    aes(x=skill_count, y=med)) +
  geom_col(
    aes(
      fill = ifelse(med < 3519, 'red', 'blue')
      )
    )+
  geom_hline(linetype= 1,
             yintercept = 3519
             )+
  labs(
    title = "Mediaan salaris Indeed vs. aantal skills",
    subtitle = "HoB 3de jaars salaris als referentie lijn",
    y = "Mediaan salaris Indeed",
    x = "Aantal skills"
  ) +
  theme(
    legend.position= "none"
  )

```

Bovenstaande analyse zou nog verdiept kunnen worden in een vervolg
onderzoek door het aantal jaren werkervaring mee te nemen. Dit levert nu
echter te weinig observaties (11) per categorie op.

### In hoeverre sluit het "curriculum" van HoB zich aan bij de ontwikkelende markt? {.tabset}

Deze deelvraag behoeft twee analyses. Enerzijds is het van belang om te
weten hoe het huidige aanbod van HoB aansluit op de huidige vraag.
Daarnaast is het van belang om te weten hoe de vraag naar Skills zich
ontwikkelt. Binnen dit onderzoek is het laatste lastig te realiseren.
Een goede trend analyse zal \~3 jaar aan data behoeven. Hierin schiet de
dataset te kort, dit zal impact hebben op de analyse.

Het eerste waar naar gekeken wordt, de huidige vraag naar skills.
Hieronder zijn twee grafieken te vinden. De eerste grafiek geeft de top
20 vaardigheden weer, de tweede grafiek de minst gevraagde 20.

#### top skills

```{r}

all_skills <- df_indeed_skills$skills %>% sort(decreasing= TRUE)

# create a top and bottom 20
top_skills <- table(df_indeed_skills$skills) %>% sort() %>% tail(20)
bottom_skills <- table(df_indeed_skills$skills) %>% sort() %>% head(20)

# plot top skills
df_indeed_skills %>%
    # filter df based on most sought skill
  filter(skills %in% names(top_skills)) %>% 
  ggplot(
      aes(
        x = reorder(
          skills,
          skills,
          function(x) + length(x) # decreasing order skill
          ),
          fill= job_type
          )
      ) +
  geom_bar() +
  theme(
    axis.text.y = element_text(
      face= 'bold',
      size= 14)) + # Rotate x-labels and change font
      coord_flip() +
  labs(
    x= "Skill"
  )


```

#### Bottom skills

```{r}

# plot bottom skills
df_indeed_skills %>%
    # filter df based on least sought skill
  filter(skills %in% names(bottom_skills)) %>% 
  ggplot(
      aes(
        x = reorder(
          skills,
          skills,
          function(x) + length(x) # decreasing order skill
          ),
          fill= job_type
          )
      ) +
  geom_bar() +
  theme(
    axis.text.y = element_text(
      face= 'bold',
      size= 14)) + # Rotate x-labels and change font
      coord_flip() +
  labs(
    x= "Skill"
  )
```

Zoals gesteld zijn de consultants in de eerste 3 jaar voornamelijk bezig
met de skills data modeling, excel, R, Python, SQL en PowerBI. Zoals te
zien sluit HoB met vier van de skills aan bij de meest gewilde skills.
Ook wordt er ingezet op cloud oplossingen binnen HoB wat aansluit op de
gevraagde skills "azure" en "aws". Tableau is een onverwachte
vaardigheid, in plaats daarvan was de verwachting namelijk dat PowerBI
in de top zou staan hier zit dus een gat in vraag en aanbod. Daarnaast
wordt duidelijk dat er op het gebied van data science een gat bestaat
tussen HoB en de markt. Zo zijn data science vaardigheden (ai, machine
learning, spark, scala, go) niet direct onderdeel van het curriculum.

De minst gevraagde skills zijn voor dit onderzoek minder relevant. Toch
is het goed om kaart te hebben welke skills het minst gevraagd worden.
Dit zijn voornamelijk skills om te vermijden.

Interessant om nog op te merken dat het voornaamste aandeel van
gevraagde skills zich rond de top 5 bevindt. Dit zijn dan ook gebieden
waar HoB zich het best op kan focussen om relevant te blijven voor de
markt. Wilt HoB uitbreiden richting minder gevraagde maar toch relevante
skills? Dan is een tweede stap om ontwikkeling te bieden in de top 20.
Dit kan voor programmeertalen en frameworks een cursus zijn, voor
concepten zoals "rest" (API's) zijn dit informatie avonden.

### Ontwikkeling skill behoeften {.tabset}

De vraag "in hoeverre sluit het "curiculum" van HoB aan op de
ontwikkelende markt" is in de ten delen beantwoord. Het is duidelijk wat
de huidige vraag is. Met de huidige ontwikkelingen op het gebied van big
data en cloud technologie die in de praktijk zichtbaar zijn is het
aannemelijk dat deze top 20 skills ook belangrijk zullen zijn om verder
in de gaten te gaan houden.

Om een sterke trend analyse te doen is minstens \~3 jaar aan data nodig.
Dit is een beperking die verder in het hoofdstuk "discussie" wordt
besproken. Wat wel aangeleverd kan worden is de functionaliteit om deze
analyse op een later moment te doen.

Onderstaande grafieken, met de verloop per skill, en de bijbehorende
code generen de inzichten om een simpele trend analyse te doen. In
onderstaande figuren wordt duidelijk dat er momenteel een neerwaartse
trend is, de aannamen is dat dit wanneer er meer data beschikbaar is
door scrapen de trend een positief verloop laat zien.

De onderstaande grafieken geven de dagelijks vraag naar skills aan en
geven de algehele trend weer. Op het moment van schrijven zijn dit enkel
negatieve trends. De aannamen is dat, na verloop van tijd, wanneer meer
data gescraped wordt deze skills in iedergeval een positieve trend
zullen laten zien.

#### SQL

```{r SQL instroom}
plot_indeed_timeseries_data(df_indeed_skills_date_filtered, "sql")
```

#### Python

```{r Python instroom}
plot_indeed_timeseries_data(df_indeed_skills_date_filtered, "python")
```

#### R

```{r R instroom}
plot_indeed_timeseries_data(df_indeed_skills_date_filtered, "r")

```

#### Tableau

```{r Tableau instroom}
plot_indeed_timeseries_data(df_indeed_skills_date_filtered, "tableau")
```

#### Azure

```{r Azure instroom}
plot_indeed_timeseries_data(df_indeed_skills_date_filtered, "azure")
```

#### AWS

```{r AWS instroom}
plot_indeed_timeseries_data(df_indeed_skills_date_filtered, "aws")
```

### Welke opdrachtgevers worden nog niet bediend door HoB? {.tabset}

Voor deze deelvraag was het naast de huidige situatie ook de bedoeling
een trend analyse te doen. Helaas is duidelijk geworden dat er te weinig
data beschikbaar is om een betrouwbare analyse hierop te doen. Des
ondanks wordt deze deelvraag ingeleid met onderstaande twee grafieken om
daarna in te gaan op de huidige staat van de markt. De bedoeling is
hiermee alvast een aanzet te doen voor toekomstige analyses.

In de eerste grafiek hieronder is een negatieve trend te zien wanneer er
gekeken wordt naar de totale instroom aan "data" vacatures. De tweede
grafiek toont een autocorrelatie. Op de x-as zijn de dagen vanaf het
huidige datapunt gemeten (0 is dezelfde dag). Op de y-as is de mate van
correlatie. Alle punten boven of onder de blauwe lijnen zouden statisch
significant moeten zijn. Op het moment van schrijven lijkt er een
significant patroon op te treden iedere 6 dagen. Echter is de correlatie
slechts 0.3 wat aanduidt dat het patroon erg zwak is.

#### Instroom vanaf 5 februari 2022

```{r filtered date counts}

# filter data to contain only points after 5th of february


plot_indeed_timeseries_data(df_indeed_final_date_filtered)

```

#### Auto-correlatie instroom

```{r}
#create listing date table
listing_date_table <- table(df_indeed_final_date_filtered$listing_date)

#calculate autocorrelation
acf(listing_date_table, main= "Instroom autocorrelatie")
```

Hoewel de dataset te klein is om momenteel een gegronde analyse te doen
zijn bovenstaande grafieken en code zeker bruikbaar voor een latere
analyse indien gewenst.

### Ontwikkeling opdrachtgevers {.tabset}

De laatste deelvraag binnen dit onderzoek richt zich op de mogelijke
opdrachtgevers. Hiervan was het oorspronkelijk de bedoeling om een trend
analyse te doen bij opdrachtgevers. Echter is dit door het tekort aan
data niet te realiseren. In plaats daarvan wordt hier gekeken naar de
huidige gap tussen de organisaties die vacatures uit hebben staan op
Indeed en de organisaties die HoB bedient.

Allereerst wordt in de eerste hieronder de top 25 bedrijven getoond op
basis van het aantal geplaatste vacatures vanaf de eerste datum in de
dataset. Grofweg 50% van deze organisaties betreffen detacheerders
(waaronder House of Bèta). Dit is een interessant gegeven, de aannamen
hier is namelijk dat de reden dat er zoveel vraag vanuit detacheerders
en consultants is komt door de grote vraag naar data professionals in
het algemeen.

Buiten de bevestiging van de grote vraag naar data professionals is het
vooral interessant om te kijken naar de organisaties met de hoogste
vraag die niet vallen onder consultancy of detacheerders. In de tweede
grafiek is een subset gemaakt van deze 25 bedrijven zonder consultancies
of detacheerders. Uit deze grafiek zijn op dit moment 2 partijen waar
HoB geen personeel aan levert: Bertelsmann SE & Co, en Amarant.

Uiteraard is dit een summiere analyse om een daadwerkelijke gap aan te
tonen vandaar nog twee aanvullende analyses. De derde grafiek laat zien
dat voornamelijk python, sql, R, ai en (neural) network vaardigheden
veel gezocht wordt door de subset van bedrijven. De tweede grafiek laat
eenzelfde verdeling zien maar dan vanuit een andere invalshoek. De
eerste grafiek beschouwt de meest gevraagde skills vanuit de hele
onderzoeks populatie. De vierde grafiek daarentegen laat toont alleen de
top skills die daadwerkelijk door deze bedrijven worden gezocht. Uit
deze laatste grafiek komen drie inzichten naar voren. De belangrijkste
skills blijven ongewijzigd, SAS (een programeertaal waar HoB zich sinds
kort op richt) komt naar voren op de 5e plek. De overige 5 vaardigheden
hebben veelal te maken met big data toepassingen.

#### top 20 bedrijven - functie types

```{r}

top_companies <- table(df_indeed_final$Company) %>% sort() %>% tail(25)

df_indeed_final %>%
  filter(Company %in% names(top_companies)) %>%
  ggplot(
    aes(
        x = reorder(
          Company,
          Company,
          function(x) + length(x) # decreasing order company
        ),
        fill= job_type
    )
  ) +
  geom_bar() +
  coord_flip() +
  labs(
    titel= "Aantal vacatures per bedrijf per funtietype",
    x= "Bedrijf"
    )
```

#### top 10 bedrijven - functie types

```{r}
# get top 10 companies not being consultancies and others alike
companies_of_interest <- c(
  "Bertelsmann SE & Co. KGaA - Corporate Center",
  "NN Group",
  "Rabobank",
  "belastingdienst",
  "Rijkswaterstaat",
  "Alliander",
  "MN",
  "ING",
  "Amarant",
  "ABN AMRO"
)

df_indeed_final %>%
  filter(Company %in% companies_of_interest) %>%
  ggplot(
    aes(
        x = reorder(
          Company,
          Company,
          function(x) + length(x) # decreasing order company
        ),
        fill= job_type
    )
  ) +
  geom_bar() +
  coord_flip() +
  labs(
    titel= "Aantal vacatures per bedrijf per funtietype",
    x= "Bedrijf"
    )
```

#### top 10 bedrijven - top 10 skills (gehele populatie)

```{r}


important_skills_full_pop <- table(df_indeed_skills$skills) %>% sort() %>% tail(10)

df_indeed_skills_date_filtered %>%
  filter(
    Company %in% companies_of_interest &
    skills %in% names(important_skills_full_pop)) %>%
  ggplot(
    aes(
        x = reorder(
          skills,
          skills,
          function(x) + length(x) # decreasing order company
        ),
        fill= Company
    )
  ) +
  geom_bar() +
  coord_flip() +
  labs(
    titel= "Aantal vacatures per bedrijf per funtietype",
    x= "Bedrijf"
    )
```

#### Top 10 bedrijven - top 10 skills

```{r}

df_important_companies <- df_indeed_skills_date_filtered %>%
  filter(Company %in% companies_of_interest)

important_skills <- table(df_important_companies$skills) %>%
  sort() %>%
  tail(10)

df_indeed_skills_date_filtered %>%
  filter(
    Company %in% companies_of_interest &
    skills %in% names(important_skills)) %>%
  ggplot(
    aes(
        x = reorder(
          skills,
          skills,
          function(x) + length(x) # decreasing order company
        ),
        fill= Company
    )
  ) +
  geom_bar() +
  coord_flip() +
  labs(
    titel= "Aantal vacatures per bedrijf per funtietype",
    x= "Bedrijf"
    )
```

Een kanttekening die hierbij moet worden gemaakt is dat Bertelsmann SE &
Co. Deze uitslag lijkt te beïnvloeden door hun sterke aanwezigheid (NN
heeft dit effect minder).

Om deze deelvraag kort samen te vatten: HoB heeft een kans om bij
Bertelsmann SE &Co. en Amarant nieuwe kansen te creëren. Om haar positie
bij partijen te versterken is het naast bestaande cursussen nog
waardevol om te kijken naar big data en gevorderde data science skills.

## Discussie

In dit hoofdstuk komen de verbeterpunten en tekortkoming aanbod. Ook
wordt er aandacht besteed aan de deployement en wat er daadwerkelijk van
waarde wordt opgeleverd.

### Data omvang

Allereerst de omvang van de dataset. Een groot deel van dit onderzoek is
beantwoord met de beschikbare data. Echter zijn verdiepingen op trend
analyses achterwegen gelaten alsmede analyses over meerdere dimensies
waarvoor de dataset te klein is. Bij aanvang van dit project was de
inschatting van de hoeveelheid data die binnengehaald kon worden niet
direct duidelijk. Het voornaamste verbeterpunt is om de dataset uit te
breiden voor verdere analyses. Een leerpunt is dat analyses over drie
dimensies een dataset erg zullen versnipperen, hier moet volgende keer
rekening mee gehouden worden bij het beoordelen van het aantal nodige
observaties.

### Data kwaliteit

Datakwaliteit is al kort aangestipt door het project heen. Het ontbreken
van een API en de tijdsdruk voor dit onderzoek heeft betekend dat er
keuzes zijn gemaakt ik en het scrapen, schonen en vereiken van de data.
De belangrijkste om hier te benoemen zijn: functie type en vaardigheden.

Allereerst zijn er fouten aanwezig in de functie type. Er is onderscheid
gemaakt tussen data analist, data engineers, en data scientisten. Hoewel
er vacatures kunnen zijn voor hele andere data functies is iedere
functie geforceerd in een van deze catagorieën.

De tweede kwaliteit issue zit in de vaardigheden. Een goed voorbeeld is
de vaardigheid "network". Dit had waarschijnlijk "neural network" moeten
zijn. Wanneer er gezocht wordt op "power bi" zal "powerbi" niet worden
gevonden hierdoor mist er data. Ook zijn skills zoals "sql" in veel
verschillende varianten te vinden zoals "mysql". Deze skills zijn apart
opgeslagen, dit geeft een vertekende weergaven van de vraag naar de
echte skills.

### Data scraping

Als laatste zijn er twee kanttekeningen bij het data scrapen. Allereerst
is alleen Indeed als bron gebruikt. Qua website omvang is dit misschien
een goede representatie maar het zou kunnen dat hier nog een beter
alternatief voor is. Het gevaar van dubbele vacatures binnenhalen bij
het gebruik van meerdere scrapers is dan natuurlijk een gevaar. Wegens
tijdsgebrek is hier geen verder onderzoek.

Het tweede punt betreft de query. De scraper is gericht op een zo breed
mogelijk profiel. Om deze reden is als zoekopdracht "data" gebruikt. Een
voorgestelde verbetering is de scraper om te bouwen naar een generieke
scraper. Hierbij kan de query vrij worden ingevuld en opgeslagen in een
variabele die ook wordt opgeslagen in de dataset. Voor dit onderzoek zou
dit hebben geholpen om accurater te zoeken naar analisten, engineers, en
scientisten. Dit sluit aan op de verdere kritiek onder "data kwaliteit".

### Deployment

Als laatste deployement. Bij dit onderzoek worden een aantal artefacten
aangeleverd die gebruikt kunnen worden voor ad-hoc analyses. De scripts
en code zijn zonder de markdown te gebruiken en makkelijk te bewerken.
De code is voorzien van commentaar en kan relatief makelijk worden
uitgebreid met nieuwe functionaliteiten waar nodig.

| Artefact                                         | Gebruik                                                                                                  |
|------------------------|------------------------------------------------|
| rmd_Capstone_DMAE_MaelvDijk.Rmd                  | Markdown document met de analyses voor dit onderzoek.                                                    |
| Data_cleaned / Data_raw                          | diverse .CSV bestanden met brondata.                                                                     |
| indeed_scraper.R (Supporting_code)               | webscraper voor de Indeed vacature website. Te gebruiken om nieuwe data binnen te halen.                 |
| Scraped_indeed_data_wrangler.R (Supporting_code) | script om de gescrapte data (opgeslagen in .csv format) op te schonen en verijken met nieuwe datapunten. |
| Scraped_indeed_plotter.R (Supporting_code)       | script om de gescrapte data na schonen en verrijken op een uniforme wijze te plotten.                    |

## Conclusie

Als laatste het antwoord op de vraag: "Hoe positioneert HoB zich ten
opzichten van de Nederlandse arbeidsmarkt op het gebied van data
professionals kijkend naar loon, vaardigheden en potentiële
opdrachtgevers?"

Het is duidelijk geworden data HoB een competatief salaris biedt aan
haar medewerkers,In iedergeval vanaf jaar 3 waar zij meer dan 50% van de
vacatures in loon voorbijgaan. Voorzowel jaar 1, 2, en 3 verdient een
HoB consultant boven modaal. Kijkende naar de gehele onderzoeks
populatie.

Op het gebied van vaardigheden en opdrachtgevers doet HoB het erg goed.
Zij bedient een groot deel van de bedrijven die de meeste vacatures uit
hebben staan. Hierin bestaat voor HoB de kans om met Amarant en
Bertelsmann zaken te gaan doen. Wat betrecft de vaardigheden is het
belangrijk dat HoB blijft inzetten op de belangrijkste vaardigheden
(sql, python en r) maar ook de verdieping aanbiedt op het gebied van
data science (ai en neurale netwerken) en big data oplossingen (scala,
spark en hive).

Kortom HoB lijkt competitief zowel op loon als aanbod waarbij er kansen
bestaan om vooral naar opdrachtgevers toe verder uit te breiden in zowel
vaardigheden als partners.

## Addendum

------------------------------------------------------------------------

------------------------------------------------------------------------

------------------------------------------------------------------------

------------------------------------------------------------------------

------------------------------------------------------------------------

```{r Indeed salaris verdeling per skill, echo=FALSE}

df_indeed_rel_skill <- df_indeed_final[df_indeed_final$skill_count %in% c(3,4,5), ]

df_indeed_rel_skill %>%
    ggplot(aes(x= salary)) +
      geom_density(alpha=0.5) +
      geom_segment(
        aes(
          x =2763, xend =2763,
          y= 0.0000, yend=0.0001,
          linetype= "HoB 1ste jaar"
        )
      ) +
        geom_segment(
        aes(
          x =3087, xend =3087,
          y= 0.0000, yend=0.0001,
          linetype= "HoB 2de jaar"
        )
      ) +
        geom_segment(
        aes(
          x =3519, xend =3519,
          y= 0.0000, yend=0.0001,
          linetype= "HoB 3de jaar"
        )
      )+
      facet_wrap(vars(skill_count)) +
      labs(
        title="Verdeling geboden salaris per aantal gevraagde skills",
        y= "Frequentie (%)",
        x= "Geboden salaris (\u20ac)"
      ) +
      theme(
        plot.title=element_text(hjust=0.5))+
      scale_y_continuous(
        breaks= seq(0.0000, 0.0007, 0.0001),
        labels= seq(0.00, 0.07, 0.01)
      )
```

Hieronder wordt de data verkend. D001 is al een beetje geschoond en kan
hieronder makkelijk verkend worden. D002 bevat een ruwe data die niet
goed te gebruiken is. Hiervoor is het script
*'./supporting_code/Scraped_indeed_data_wrangler.R'* beschikbaar. Dit
script bevat functies om de data vanuit de scraper op te schonen zodat
er wel inzichten uit de data gehaald kunnen worden.

#### D001

Voor D001 wordt in dit onderdeel de verdeling bekeken voor het aantal
skills en salaris over de totale dataset en gesegmenteerd op baan type.

**Data Analist**

Wat gelijk opvalt aan deze dataset is dat de data analist de "minst
zware" functie lijkt. Er worden minder skills verwacht van een data
analist en het loon voor deze rol ligt ook doorgaans lager dan de andere
twee rollen.

**Data Engineer**

Het is gelijk duidelijk dat data engineers de minste vraag genieten maar
wel een zwaarder profiel moeten hebben. Het salaris van de data engineer
volgt ongeveer de verdeling van de data scientist. Wel valt op dat
ondanks de lagere hoeveelheid vacatures dat de data engineer het
zwaarste skill profiel dient te hebben.

**Data Scientist**

Naar data scientisten lijkt de hoogste vraag te zijn. Het skill profiel
van de data scientist is gemiddeld de verdeling neigt meer naar links
dan de data engineer. Ondanks dit lijkt de salaris verdeling gelijk te
zijn aan die verdeling van de data engineer.

**Andere inzichten**

Verder is in deze dataset te zien dat, wanneer er om meer skills
gevraagd wordt er een iets hoger salaris wordt geboden. Voor de hoogste
salaris groep lijkt dit echter niet te gelden. De aannamen (die nog
getoetst kan worden) is dat dit gaat om teamleiders of andere management
functies waar hard skills minder van belang zijn.

```{r D001 exploration, echo=FALSE}

df_indeed_kaggle <- read.csv(".\\Data_raw\\D001\\indeed_job_dataset.csv")

kaggle_plots <- plot_kaggle_distribution(df_indeed_kaggle)


```

### Ontwikkeling functie type en vraag opdrachtgevers {.tabset}

Ondanks dat het niet mogelijk is om nu nog een uitspraak te doen over de
ontwikkeling van vraag bij opdrachtgevers wordt er wel vast een artefact
aangeleverd in de vorm van een tijdserie op basis van functietype. Deze
grafiek en de bijbehorende code zijn hieronder en in de ondersteunende
scripts terug te vinden voor toekomstig gebruik. Omdat er nu geen
uitspraken worden gedaan i.v.m de statistische validiteit zal er geen
verdere toelichting bij de grafiek te vinden zijn.

Er is hier ook overweogen een tweede artefact mee te leveren, namelijk
een stuk code om het verloop van de vraag door individuele opdracht
gevers weer te geven. Echter zegt de grafiek door de geringe hoeveelheid
data te weinig. Om verkeerde interpetaties van de data te voorkomen
wordt deze code dan ook niet meegeleverd. Zodra er genoeg data verzamelt
is kan aan de hand van deze file en bijbehorende code een nieuw
onderdeel worden toegevoegd om deze verandering over de tijd accuraat
weer te geven.

#### tijdreeks - functie type

```{r}
df_indeed_final_date_filtered %>%
  group_by(
    listing_date,
    job_type) %>%
  summarise(
    req_per_day= n(),
    .groups= "keep") %>%
  ggplot(
    aes(
      x= listing_date,
      y= req_per_day
      )) +
  geom_line() +
  facet_wrap(
    vars(job_type),
    ncol=1) +
  stat_smooth(
    method = "lm",
    se= FALSE
    ) +
  scale_x_date(
    date_labels = "%a\n%d-%m",
    date_breaks = "week"
               )


```

## 
